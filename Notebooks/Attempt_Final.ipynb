{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures,StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.metrics import r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/train.csv')\n",
    "test = pd.read_csv('../datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exter_dict = {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\n",
    "Lot_dict = {'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3}\n",
    "Bsmt_dict = {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "Bsmt_ex_dict = {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
    "Func_dict = {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, \n",
    "             'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7}\n",
    "Fence_dict = {'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}\n",
    "\n",
    "train['Exter Qual'].replace(Exter_dict, inplace = True)\n",
    "test['Exter Qual'].replace(Exter_dict, inplace = True)\n",
    "\n",
    "train['Exter Cond'].replace(Exter_dict, inplace = True)\n",
    "test['Exter Cond'].replace(Exter_dict, inplace = True)\n",
    "\n",
    "train['Heating QC'].replace(Exter_dict, inplace = True)\n",
    "test['Heating QC'].replace(Exter_dict, inplace = True)\n",
    "\n",
    "train['Kitchen Qual'].replace(Exter_dict, inplace = True)\n",
    "test['Kitchen Qual'].replace(Exter_dict, inplace = True)\n",
    "\n",
    "train['Bsmt Qual'].replace(Bsmt_dict, inplace = True)\n",
    "test['Bsmt Qual'].replace(Bsmt_dict, inplace = True)\n",
    "\n",
    "train['Bsmt Cond'].replace(Bsmt_dict, inplace = True)\n",
    "test['Bsmt Cond'].replace(Bsmt_dict, inplace = True)\n",
    "\n",
    "train['Bsmt Exposure'].replace(Bsmt_ex_dict, inplace = True)\n",
    "test['Bsmt Exposure'].replace(Bsmt_ex_dict, inplace = True)\n",
    "\n",
    "train['Functional'].replace(Func_dict, inplace = True)\n",
    "test['Functional'].replace(Func_dict, inplace = True)\n",
    "\n",
    "train['Fireplace Qu'].replace(Bsmt_dict, inplace = True)\n",
    "test['Fireplace Qu'].replace(Bsmt_dict, inplace = True)\n",
    "\n",
    "train['Garage Qual'].replace(Bsmt_dict, inplace = True)\n",
    "test['Garage Qual'].replace(Bsmt_dict, inplace = True)\n",
    "\n",
    "train['Garage Cond'].replace(Bsmt_dict, inplace = True)\n",
    "test['Garage Cond'].replace(Bsmt_dict, inplace = True)\n",
    "\n",
    "train['Pool QC'].replace(Bsmt_dict, inplace = True)\n",
    "test['Pool QC'].replace(Bsmt_dict, inplace = True)\n",
    "\n",
    "train['Fence'].replace(Fence_dict, inplace = True)\n",
    "test['Fence'].replace(Fence_dict, inplace = True)\n",
    "\n",
    "train['Lot Shape'].replace(Lot_dict, inplace = True)\n",
    "test['Lot Shape'].replace(Lot_dict, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_vars = ['Exterior 1st','House Style', 'Central Air', 'Foundation', 'MS Zoning', 'Street', 'Alley', 'Neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns = dummy_vars)\n",
    "test = pd.get_dummies(test, columns = dummy_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Garage_Total'] = train['Garage Yr Blt'] * train['Garage Cars'] * train['Garage Area'] * train['Garage Qual'] * train['Garage Cond']\n",
    "test['Garage_Total'] = train['Garage Yr Blt'] * train['Garage Cars'] * train['Garage Area'] * train['Garage Qual'] * train['Garage Cond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Bsmt_Total'] = train['Bsmt Qual'] * train['Bsmt Cond'] * train['Bsmt Exposure'] * train['Total Bsmt SF']\n",
    "test['Bsmt_Total'] = train['Bsmt Qual'] * train['Bsmt Cond'] * train['Bsmt Exposure'] * train['Total Bsmt SF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TotRms_GrLiv'] = train['TotRms AbvGrd'] * train['Gr Liv Area']\n",
    "test['TotRms_GrLiv'] = test['TotRms AbvGrd'] * test['Gr Liv Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id_yrsld'] = train['Id'] * train['Yr Sold']\n",
    "test['id_yrsld'] = test['Id'] * test['Yr Sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Exter_Overall'] = train['Exter Qual'] * train['Overall Qual']\n",
    "test['Exter_Overall'] = test['Exter Qual'] * test['Overall Qual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice how I did it to test, not train\n",
    "features_init = [c for c in test._get_numeric_data().columns if not c in ['SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Had to remove all the NaNs for this experiment\n",
    "for n in features_init:\n",
    "    if n in train.columns:\n",
    "        train[n].fillna(0, inplace = True)\n",
    "    if n in test.columns:\n",
    "        test[n].fillna(0, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_init.remove(\"Exterior 1st_PreCast\")\n",
    "#Some of the dummy variables only appeared in test, not train, so I had to remove it to keep consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PID</th>\n",
       "      <th>MS SubClass</th>\n",
       "      <th>Lot Frontage</th>\n",
       "      <th>Lot Area</th>\n",
       "      <th>Lot Shape</th>\n",
       "      <th>Land Contour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>Lot Config</th>\n",
       "      <th>Land Slope</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_SawyerW</th>\n",
       "      <th>Neighborhood_Somerst</th>\n",
       "      <th>Neighborhood_StoneBr</th>\n",
       "      <th>Neighborhood_Timber</th>\n",
       "      <th>Neighborhood_Veenker</th>\n",
       "      <th>Garage_Total</th>\n",
       "      <th>Bsmt_Total</th>\n",
       "      <th>TotRms_GrLiv</th>\n",
       "      <th>id_yrsld</th>\n",
       "      <th>Exter_Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>533352170</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13517</td>\n",
       "      <td>2</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16894800.0</td>\n",
       "      <td>6525.0</td>\n",
       "      <td>8874</td>\n",
       "      <td>219090</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>544</td>\n",
       "      <td>531379050</td>\n",
       "      <td>60</td>\n",
       "      <td>43.0</td>\n",
       "      <td>11492</td>\n",
       "      <td>2</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20093814.0</td>\n",
       "      <td>10956.0</td>\n",
       "      <td>16976</td>\n",
       "      <td>1092896</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153</td>\n",
       "      <td>535304180</td>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7922</td>\n",
       "      <td>3</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4323942.0</td>\n",
       "      <td>9513.0</td>\n",
       "      <td>5285</td>\n",
       "      <td>307530</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>916386060</td>\n",
       "      <td>60</td>\n",
       "      <td>73.0</td>\n",
       "      <td>9802</td>\n",
       "      <td>3</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14450400.0</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>10108</td>\n",
       "      <td>639180</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255</td>\n",
       "      <td>906425045</td>\n",
       "      <td>50</td>\n",
       "      <td>82.0</td>\n",
       "      <td>14235</td>\n",
       "      <td>2</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17049384.0</td>\n",
       "      <td>5408.0</td>\n",
       "      <td>8670</td>\n",
       "      <td>512550</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id        PID  MS SubClass  Lot Frontage  Lot Area  Lot Shape  \\\n",
       "0  109  533352170           60           0.0     13517          2   \n",
       "1  544  531379050           60          43.0     11492          2   \n",
       "2  153  535304180           20          68.0      7922          3   \n",
       "3  318  916386060           60          73.0      9802          3   \n",
       "4  255  906425045           50          82.0     14235          2   \n",
       "\n",
       "  Land Contour Utilities Lot Config Land Slope  ... Neighborhood_SawyerW  \\\n",
       "0          Lvl    AllPub    CulDSac        Gtl  ...                    0   \n",
       "1          Lvl    AllPub    CulDSac        Gtl  ...                    1   \n",
       "2          Lvl    AllPub     Inside        Gtl  ...                    0   \n",
       "3          Lvl    AllPub     Inside        Gtl  ...                    0   \n",
       "4          Lvl    AllPub     Inside        Gtl  ...                    1   \n",
       "\n",
       "  Neighborhood_Somerst Neighborhood_StoneBr  Neighborhood_Timber  \\\n",
       "0                    0                    0                    0   \n",
       "1                    0                    0                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    0                    1   \n",
       "4                    0                    0                    0   \n",
       "\n",
       "   Neighborhood_Veenker  Garage_Total  Bsmt_Total TotRms_GrLiv id_yrsld  \\\n",
       "0                     0    16894800.0      6525.0         8874   219090   \n",
       "1                     0    20093814.0     10956.0        16976  1092896   \n",
       "2                     0     4323942.0      9513.0         5285   307530   \n",
       "3                     0    14450400.0      4608.0        10108   639180   \n",
       "4                     0    17049384.0      5408.0         8670   512550   \n",
       "\n",
       "  Exter_Overall  \n",
       "0            18  \n",
       "1            21  \n",
       "2            10  \n",
       "3            10  \n",
       "4            12  \n",
       "\n",
       "[5 rows x 148 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PID</th>\n",
       "      <th>MS SubClass</th>\n",
       "      <th>Lot Frontage</th>\n",
       "      <th>Lot Area</th>\n",
       "      <th>Lot Shape</th>\n",
       "      <th>Overall Qual</th>\n",
       "      <th>Overall Cond</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>Year Remod/Add</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_SawyerW</th>\n",
       "      <th>Neighborhood_Somerst</th>\n",
       "      <th>Neighborhood_StoneBr</th>\n",
       "      <th>Neighborhood_Timber</th>\n",
       "      <th>Neighborhood_Veenker</th>\n",
       "      <th>Garage_Total</th>\n",
       "      <th>Bsmt_Total</th>\n",
       "      <th>TotRms_GrLiv</th>\n",
       "      <th>id_yrsld</th>\n",
       "      <th>Exter_Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>533352170</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13517</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16894800.0</td>\n",
       "      <td>6525.0</td>\n",
       "      <td>8874</td>\n",
       "      <td>219090</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>544</td>\n",
       "      <td>531379050</td>\n",
       "      <td>60</td>\n",
       "      <td>43.0</td>\n",
       "      <td>11492</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1996</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20093814.0</td>\n",
       "      <td>10956.0</td>\n",
       "      <td>16976</td>\n",
       "      <td>1092896</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153</td>\n",
       "      <td>535304180</td>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7922</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1953</td>\n",
       "      <td>2007</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4323942.0</td>\n",
       "      <td>9513.0</td>\n",
       "      <td>5285</td>\n",
       "      <td>307530</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>318</td>\n",
       "      <td>916386060</td>\n",
       "      <td>60</td>\n",
       "      <td>73.0</td>\n",
       "      <td>9802</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>2007</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14450400.0</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>10108</td>\n",
       "      <td>639180</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255</td>\n",
       "      <td>906425045</td>\n",
       "      <td>50</td>\n",
       "      <td>82.0</td>\n",
       "      <td>14235</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1900</td>\n",
       "      <td>1993</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17049384.0</td>\n",
       "      <td>5408.0</td>\n",
       "      <td>8670</td>\n",
       "      <td>512550</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id        PID  MS SubClass  Lot Frontage  Lot Area  Lot Shape  \\\n",
       "0  109  533352170           60           0.0     13517          2   \n",
       "1  544  531379050           60          43.0     11492          2   \n",
       "2  153  535304180           20          68.0      7922          3   \n",
       "3  318  916386060           60          73.0      9802          3   \n",
       "4  255  906425045           50          82.0     14235          2   \n",
       "\n",
       "   Overall Qual  Overall Cond  Year Built  Year Remod/Add  ...  \\\n",
       "0             6             8        1976            2005  ...   \n",
       "1             7             5        1996            1997  ...   \n",
       "2             5             7        1953            2007  ...   \n",
       "3             5             5        2006            2007  ...   \n",
       "4             6             8        1900            1993  ...   \n",
       "\n",
       "   Neighborhood_SawyerW  Neighborhood_Somerst  Neighborhood_StoneBr  \\\n",
       "0                     0                     0                     0   \n",
       "1                     1                     0                     0   \n",
       "2                     0                     0                     0   \n",
       "3                     0                     0                     0   \n",
       "4                     1                     0                     0   \n",
       "\n",
       "   Neighborhood_Timber  Neighborhood_Veenker  Garage_Total  Bsmt_Total  \\\n",
       "0                    0                     0    16894800.0      6525.0   \n",
       "1                    0                     0    20093814.0     10956.0   \n",
       "2                    0                     0     4323942.0      9513.0   \n",
       "3                    1                     0    14450400.0      4608.0   \n",
       "4                    0                     0    17049384.0      5408.0   \n",
       "\n",
       "   TotRms_GrLiv  id_yrsld  Exter_Overall  \n",
       "0          8874    219090             18  \n",
       "1         16976   1092896             21  \n",
       "2          5285    307530             10  \n",
       "3         10108    639180             10  \n",
       "4          8670    512550             12  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[features_init].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features_init]\n",
    "y = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor()\n",
    "params = {\n",
    "    'max_depth': [None, 600, 200, 300, 400, 500],\n",
    "    'min_samples_split': [6,3,4,5],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['auto',30,40,50,60],\n",
    "}\n",
    "grid_forest = GridSearchCV(RandomForestRegressor(n_estimators = 100), param_grid= params, verbose= True, n_jobs= -1, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': [None, 600, 200, 300, 400, 500], 'min_samples_split': [6, 3, 4, 5], 'min_samples_leaf': [1], 'max_features': ['auto', 30, 40, 50, 60]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 300,\n",
       " 'max_features': 30,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 3}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841350158394598"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_forest.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9150947838609828"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10016.906369747347"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_train, grid_forest.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22832.435849596946"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, grid_forest.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.79021277e-03, 7.96636763e-03, 2.00636721e-03, 5.19505132e-03,\n",
       "       1.51245825e-02, 1.32263435e-03, 1.46881120e-01, 3.32913798e-03,\n",
       "       2.63310686e-02, 9.97081394e-03, 9.37751476e-03, 6.52856207e-02,\n",
       "       5.18823495e-04, 2.19547498e-02, 1.50319703e-03, 1.35639410e-02,\n",
       "       4.09594493e-04, 4.04646416e-03, 4.53129138e-02, 1.49137932e-03,\n",
       "       2.78017237e-02, 1.17540076e-02, 6.36516915e-05, 6.95630563e-02,\n",
       "       1.65591381e-03, 2.79947176e-04, 6.88072746e-03, 1.33356342e-03,\n",
       "       2.14616236e-03, 3.00900406e-04, 5.06895421e-02, 6.73321011e-03,\n",
       "       6.01544394e-04, 8.36062153e-03, 8.79503690e-03, 6.34441945e-03,\n",
       "       2.96532788e-02, 1.55780752e-02, 7.74953399e-04, 2.58732611e-03,\n",
       "       4.56340498e-03, 7.05389143e-04, 1.34770002e-04, 3.48922715e-03,\n",
       "       1.05140399e-04, 1.60400889e-04, 3.98439748e-04, 2.17045807e-04,\n",
       "       2.29012694e-03, 1.01138124e-03, 7.88450474e-05, 3.41022724e-07,\n",
       "       5.21464579e-05, 2.97026544e-04, 4.21389955e-04, 1.76586201e-04,\n",
       "       2.47738332e-04, 1.08255542e-04, 1.97898390e-04, 6.81968808e-04,\n",
       "       3.72248759e-04, 6.09397847e-05, 3.83831347e-04, 5.11839666e-06,\n",
       "       4.43211766e-04, 1.23902851e-04, 4.53735119e-05, 4.70586254e-04,\n",
       "       1.71800735e-05, 7.24638707e-05, 8.57847377e-04, 7.29863561e-04,\n",
       "       1.99376943e-04, 9.15982328e-04, 5.26551271e-04, 5.29028232e-05,\n",
       "       1.52786181e-06, 0.00000000e+00, 6.44602547e-05, 1.26755479e-04,\n",
       "       0.00000000e+00, 3.02606800e-05, 7.75597032e-04, 8.47174872e-04,\n",
       "       3.46644105e-05, 8.62449519e-05, 7.78241874e-05, 1.26686768e-04,\n",
       "       3.23797366e-05, 2.87682059e-07, 2.51037503e-05, 7.06546647e-05,\n",
       "       1.14247139e-04, 1.54129825e-04, 6.31467975e-04, 8.54359391e-04,\n",
       "       1.02539670e-04, 3.13221661e-06, 1.58038817e-04, 1.98587535e-05,\n",
       "       1.07975770e-04, 2.09626029e-04, 2.64515175e-05, 1.26168392e-04,\n",
       "       8.56032248e-04, 4.11433696e-04, 3.02008295e-04, 2.58675976e-05,\n",
       "       7.53426936e-05, 8.34674529e-05, 2.67362023e-04, 4.23915938e-04,\n",
       "       6.78094817e-05, 1.25470986e-04, 9.03844892e-02, 2.49750622e-02,\n",
       "       2.69653834e-02, 2.90324106e-03, 1.90431375e-01])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_forest.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame()\n",
    "importances['feature'] = features_init\n",
    "importances['importance'] = grid_forest.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Exter_Overall</td>\n",
       "      <td>1.904314e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overall Qual</td>\n",
       "      <td>1.468811e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Garage_Total</td>\n",
       "      <td>9.038449e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gr Liv Area</td>\n",
       "      <td>6.956306e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Exter Qual</td>\n",
       "      <td>6.528562e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Kitchen Qual</td>\n",
       "      <td>5.068954e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Total Bsmt SF</td>\n",
       "      <td>4.531291e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Garage Cars</td>\n",
       "      <td>2.965328e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1st Flr SF</td>\n",
       "      <td>2.780172e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>TotRms_GrLiv</td>\n",
       "      <td>2.696538e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Year Built</td>\n",
       "      <td>2.633107e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Bsmt_Total</td>\n",
       "      <td>2.497506e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bsmt Qual</td>\n",
       "      <td>2.195475e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Garage Area</td>\n",
       "      <td>1.557808e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lot Area</td>\n",
       "      <td>1.512458e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BsmtFin SF 1</td>\n",
       "      <td>1.356394e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2nd Flr SF</td>\n",
       "      <td>1.175401e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Year Remod/Add</td>\n",
       "      <td>9.970814e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mas Vnr Area</td>\n",
       "      <td>9.377515e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Fireplace Qu</td>\n",
       "      <td>8.795037e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Fireplaces</td>\n",
       "      <td>8.360622e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PID</td>\n",
       "      <td>7.966368e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Full Bath</td>\n",
       "      <td>6.880727e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TotRms AbvGrd</td>\n",
       "      <td>6.733210e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Garage Yr Blt</td>\n",
       "      <td>6.344419e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lot Frontage</td>\n",
       "      <td>5.195051e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Open Porch SF</td>\n",
       "      <td>4.563405e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bsmt Unf SF</td>\n",
       "      <td>4.046464e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Screen Porch</td>\n",
       "      <td>3.489227e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Overall Cond</td>\n",
       "      <td>3.329138e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Neighborhood_Gilbert</td>\n",
       "      <td>1.025397e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Street_Pave</td>\n",
       "      <td>8.624495e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Neighborhood_SawyerW</td>\n",
       "      <td>8.346745e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Exterior 1st_AsbShng</td>\n",
       "      <td>7.884505e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Alley_Grvl</td>\n",
       "      <td>7.782419e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Neighborhood_Sawyer</td>\n",
       "      <td>7.534269e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>House Style_SLvl</td>\n",
       "      <td>7.246387e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Neighborhood_BrkSide</td>\n",
       "      <td>7.065466e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Neighborhood_Timber</td>\n",
       "      <td>6.780948e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>MS Zoning_C (all)</td>\n",
       "      <td>6.446025e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Low Qual Fin SF</td>\n",
       "      <td>6.365169e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Exterior 1st_WdShing</td>\n",
       "      <td>6.093978e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Foundation_Slab</td>\n",
       "      <td>5.290282e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Exterior 1st_BrkComm</td>\n",
       "      <td>5.214646e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>House Style_2.5Unf</td>\n",
       "      <td>4.537351e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Street_Grvl</td>\n",
       "      <td>3.466441e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Neighborhood_Blmngtn</td>\n",
       "      <td>3.237974e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>MS Zoning_RH</td>\n",
       "      <td>3.026068e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Neighborhood_NPkVill</td>\n",
       "      <td>2.645152e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Neighborhood_SWISU</td>\n",
       "      <td>2.586760e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Neighborhood_BrDale</td>\n",
       "      <td>2.510375e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Neighborhood_MeadowV</td>\n",
       "      <td>1.985875e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>House Style_SFoyer</td>\n",
       "      <td>1.718007e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>House Style_1.5Unf</td>\n",
       "      <td>5.118397e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Neighborhood_Greens</td>\n",
       "      <td>3.132217e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Foundation_Stone</td>\n",
       "      <td>1.527862e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Exterior 1st_AsphShn</td>\n",
       "      <td>3.410227e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Neighborhood_Blueste</td>\n",
       "      <td>2.876821e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>MS Zoning_I (all)</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Foundation_Wood</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature    importance\n",
       "118         Exter_Overall  1.904314e-01\n",
       "6            Overall Qual  1.468811e-01\n",
       "114          Garage_Total  9.038449e-02\n",
       "23            Gr Liv Area  6.956306e-02\n",
       "11             Exter Qual  6.528562e-02\n",
       "..                    ...           ...\n",
       "76       Foundation_Stone  1.527862e-06\n",
       "51   Exterior 1st_AsphShn  3.410227e-07\n",
       "89   Neighborhood_Blueste  2.876821e-07\n",
       "80      MS Zoning_I (all)  0.000000e+00\n",
       "77        Foundation_Wood  0.000000e+00\n",
       "\n",
       "[119 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances.sort_values('importance', ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsurprisingly, Random Forests work great. Let's try some Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1538, 119)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NICK\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\NICK\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\NICK\\Anaconda3\\envs\\dsi\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "drop_rate = 0.15\n",
    "model.add(Dense(256, activation = 'relu', input_shape = (X_train_ss.shape[1],)))\n",
    "model.add(Dropout(rate = drop_rate))\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(rate = drop_rate))\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(rate = drop_rate))\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(rate = drop_rate))\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(rate = drop_rate))\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(rate = drop_rate))\n",
    "\n",
    "\n",
    "# model.add(Dense(64, activation = 'relu'))\n",
    "# model.add(Dropout(rate = drop_rate))\n",
    "\n",
    "# model.add(Dense(64, activation = 'relu'))\n",
    "# model.add(Dropout(rate = drop_rate))\n",
    "\n",
    "model.add(Dense(1, activation = None))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1538 samples, validate on 513 samples\n",
      "Epoch 1/1000\n",
      "1538/1538 [==============================] - 8s 5ms/step - loss: 39107572761.3004 - val_loss: 39515103575.3294\n",
      "Epoch 2/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 39106038226.0598 - val_loss: 39512149672.6706\n",
      "Epoch 3/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 39102148927.5839 - val_loss: 39504692253.9415\n",
      "Epoch 4/1000\n",
      "1538/1538 [==============================] - 0s 80us/step - loss: 39092533016.3017 - val_loss: 39485751176.2339\n",
      "Epoch 5/1000\n",
      "1538/1538 [==============================] - 0s 81us/step - loss: 39069124986.1743 - val_loss: 39439256889.3879\n",
      "Epoch 6/1000\n",
      "1538/1538 [==============================] - 0s 84us/step - loss: 39011943028.5150 - val_loss: 39332277984.5614\n",
      "Epoch 7/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 38884496132.3277 - val_loss: 39101066633.2320\n",
      "Epoch 8/1000\n",
      "1538/1538 [==============================] - 0s 79us/step - loss: 38603313165.3160 - val_loss: 38598967583.4386\n",
      "Epoch 9/1000\n",
      "1538/1538 [==============================] - 0s 84us/step - loss: 38005105428.3069 - val_loss: 37568934910.0039\n",
      "Epoch 10/1000\n",
      "1538/1538 [==============================] - 0s 81us/step - loss: 36750821224.1977 - val_loss: 35537709163.7895\n",
      "Epoch 11/1000\n",
      "1538/1538 [==============================] - 0s 91us/step - loss: 34455281791.8335 - val_loss: 31754314694.1131\n",
      "Epoch 12/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 30122038209.4148 - val_loss: 25210717728.9357\n",
      "Epoch 13/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 22965650545.1860 - val_loss: 15435849981.5049\n",
      "Epoch 14/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 13142236222.5852 - val_loss: 5485161934.0975\n",
      "Epoch 15/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 5786980120.9675 - val_loss: 5131079356.6316\n",
      "Epoch 16/1000\n",
      "1538/1538 [==============================] - 0s 79us/step - loss: 8322432171.7763 - val_loss: 4849030676.9591\n",
      "Epoch 17/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 5099673996.9831 - val_loss: 2151645220.6784\n",
      "Epoch 18/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 3446770491.5891 - val_loss: 2311641935.7505\n",
      "Epoch 19/1000\n",
      "1538/1538 [==============================] - 0s 89us/step - loss: 3481172699.7139 - val_loss: 2426242657.3099\n",
      "Epoch 20/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 3355750266.3407 - val_loss: 1915035906.8694\n",
      "Epoch 21/1000\n",
      "1538/1538 [==============================] - 0s 83us/step - loss: 2559642918.9493 - val_loss: 1523514653.9176\n",
      "Epoch 22/1000\n",
      "1538/1538 [==============================] - 0s 80us/step - loss: 2397657426.2263 - val_loss: 1502956226.8070\n",
      "Epoch 23/1000\n",
      "1538/1538 [==============================] - 0s 86us/step - loss: 2572374106.7568 - val_loss: 1567981263.4698\n",
      "Epoch 24/1000\n",
      "1538/1538 [==============================] - 0s 82us/step - loss: 2655967334.5332 - val_loss: 1261534304.7485\n",
      "Epoch 25/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 2175720195.6619 - val_loss: 1871022161.5906\n",
      "Epoch 26/1000\n",
      "1538/1538 [==============================] - 0s 89us/step - loss: 2713073578.7776 - val_loss: 2225574828.4133\n",
      "Epoch 27/1000\n",
      "1538/1538 [==============================] - 0s 87us/step - loss: 2838714089.0299 - val_loss: 1819015019.1657\n",
      "Epoch 28/1000\n",
      "1538/1538 [==============================] - 0s 84us/step - loss: 2373729321.2796 - val_loss: 1290150422.5497\n",
      "Epoch 29/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1928365292.6918 - val_loss: 1145816444.4288\n",
      "Epoch 30/1000\n",
      "1538/1538 [==============================] - 0s 87us/step - loss: 1957357071.3134 - val_loss: 1121925608.8577\n",
      "Epoch 31/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1974025498.2991 - val_loss: 1066128958.0741\n",
      "Epoch 32/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1890182737.3524 - val_loss: 1240614467.2125\n",
      "Epoch 33/1000\n",
      "1538/1538 [==============================] - 0s 79us/step - loss: 1926449563.9948 - val_loss: 1362173895.4854\n",
      "Epoch 34/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 2017248720.0624 - val_loss: 1218366304.5614\n",
      "Epoch 35/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1801244781.1912 - val_loss: 968601975.1423\n",
      "Epoch 36/1000\n",
      "1538/1538 [==============================] - 0s 78us/step - loss: 1918332637.8778 - val_loss: 1549160548.5536\n",
      "Epoch 37/1000\n",
      "1538/1538 [==============================] - 0s 74us/step - loss: 2687206807.4694 - val_loss: 1519102904.6394\n",
      "Epoch 38/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 2344971595.5683 - val_loss: 1010558663.1735\n",
      "Epoch 39/1000\n",
      "1538/1538 [==============================] - 0s 81us/step - loss: 1812506125.9818 - val_loss: 1056073852.8733\n",
      "Epoch 40/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1799765853.7945 - val_loss: 1256984819.4620\n",
      "Epoch 41/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1933161631.2926 - val_loss: 1131999223.2359\n",
      "Epoch 42/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1816257069.9402 - val_loss: 922634164.8733\n",
      "Epoch 43/1000\n",
      "1538/1538 [==============================] - 0s 85us/step - loss: 1671652047.3966 - val_loss: 952932585.6296\n",
      "Epoch 44/1000\n",
      "1538/1538 [==============================] - 0s 82us/step - loss: 1841189504.8322 - val_loss: 991446573.5984\n",
      "Epoch 45/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1811602809.6749 - val_loss: 892228201.2688\n",
      "Epoch 46/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 1668582407.9896 - val_loss: 905166753.6218\n",
      "Epoch 47/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 1655217175.3030 - val_loss: 990241457.7778\n",
      "Epoch 48/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1606238204.2549 - val_loss: 931422189.7856\n",
      "Epoch 49/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1553758097.1443 - val_loss: 915997962.2924\n",
      "Epoch 50/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1755084848.2705 - val_loss: 992673269.1462\n",
      "Epoch 51/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1667239113.9038 - val_loss: 942137440.0000\n",
      "Epoch 52/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1618380577.5397 - val_loss: 875221132.6004\n",
      "Epoch 53/1000\n",
      "1538/1538 [==============================] - 0s 84us/step - loss: 1690345730.0806 - val_loss: 849213976.4133\n",
      "Epoch 54/1000\n",
      "1538/1538 [==============================] - 0s 79us/step - loss: 1596999486.6684 - val_loss: 852825190.1209\n",
      "Epoch 55/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1443419911.3030 - val_loss: 854809097.9006\n",
      "Epoch 56/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1692736877.9610 - val_loss: 839658207.4542\n",
      "Epoch 57/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1371436982.1795 - val_loss: 839448117.7232\n",
      "Epoch 58/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1566756839.2822 - val_loss: 831758733.5673\n",
      "Epoch 59/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1406667708.2133 - val_loss: 822431281.2320\n",
      "Epoch 60/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1634772291.0793 - val_loss: 817337350.7836\n",
      "Epoch 61/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1469081377.9974 - val_loss: 809230123.2125\n",
      "Epoch 62/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1535128030.3771 - val_loss: 800804235.9454\n",
      "Epoch 63/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1456840072.4889 - val_loss: 789926499.9766\n",
      "Epoch 64/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1441921421.3570 - val_loss: 822809762.7495\n",
      "Epoch 65/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1529835966.0858 - val_loss: 825049820.3041\n",
      "Epoch 66/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1615538249.5709 - val_loss: 814120255.0994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1507052170.4863 - val_loss: 763134323.3099\n",
      "Epoch 68/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1449655455.7919 - val_loss: 793547156.3665\n",
      "Epoch 69/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1363635158.8869 - val_loss: 840535973.7700\n",
      "Epoch 70/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1441527569.6437 - val_loss: 787526691.3684\n",
      "Epoch 71/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1589198349.9818 - val_loss: 765875370.3392\n",
      "Epoch 72/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1436099984.6450 - val_loss: 867889176.4522\n",
      "Epoch 73/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1465150041.3836 - val_loss: 907724052.1481\n",
      "Epoch 74/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1547358049.2068 - val_loss: 821789055.3762\n",
      "Epoch 75/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1433154752.3329 - val_loss: 786941136.6667\n",
      "Epoch 76/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1522907546.4655 - val_loss: 826490346.8582\n",
      "Epoch 77/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1481211764.0156 - val_loss: 794406614.7368\n",
      "Epoch 78/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1358902382.8557 - val_loss: 820148189.0682\n",
      "Epoch 79/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1393284923.5891 - val_loss: 896738206.1287\n",
      "Epoch 80/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1593547085.5657 - val_loss: 900224977.0916\n",
      "Epoch 81/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1548640834.2471 - val_loss: 1021482531.9298\n",
      "Epoch 82/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1547902518.9285 - val_loss: 954879228.2573\n",
      "Epoch 83/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1475497795.8700 - val_loss: 815976988.1949\n",
      "Epoch 84/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1419141271.5943 - val_loss: 817679592.9513\n",
      "Epoch 85/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1428151877.9922 - val_loss: 832150401.1384\n",
      "Epoch 86/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1359865413.9090 - val_loss: 813588022.3314\n",
      "Epoch 87/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1351174498.0806 - val_loss: 835666529.0604\n",
      "Epoch 88/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1454693990.2003 - val_loss: 855436210.5263\n",
      "Epoch 89/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1320352363.9740 - val_loss: 911570323.0877\n",
      "Epoch 90/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1458577254.6996 - val_loss: 893551975.7973\n",
      "Epoch 91/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1407582669.1495 - val_loss: 802715794.9006\n",
      "Epoch 92/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1383564060.2133 - val_loss: 751896113.3879\n",
      "Epoch 93/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1308960220.0000 - val_loss: 757934882.5653\n",
      "Epoch 94/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1336260317.8179 - val_loss: 739580938.8070\n",
      "Epoch 95/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1367786805.1808 - val_loss: 735166390.6745\n",
      "Epoch 96/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1380373244.6710 - val_loss: 730989216.8421\n",
      "Epoch 97/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1315486324.2653 - val_loss: 726434810.1209\n",
      "Epoch 98/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1315395879.3654 - val_loss: 728707005.3489\n",
      "Epoch 99/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1296945145.1131 - val_loss: 777613626.2612\n",
      "Epoch 100/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1346388659.2874 - val_loss: 771908166.0507\n",
      "Epoch 101/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1340229607.0325 - val_loss: 745617345.0604\n",
      "Epoch 102/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1369796085.3472 - val_loss: 747429718.8928\n",
      "Epoch 103/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1337936335.8544 - val_loss: 757179581.8791\n",
      "Epoch 104/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1348135361.1860 - val_loss: 742017026.0585\n",
      "Epoch 105/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1271328017.3108 - val_loss: 729790219.1969\n",
      "Epoch 106/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1377992744.6138 - val_loss: 733919865.8869\n",
      "Epoch 107/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1338509608.2809 - val_loss: 751003251.7115\n",
      "Epoch 108/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1340882278.2003 - val_loss: 749576224.1871\n",
      "Epoch 109/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1354816702.4915 - val_loss: 752771809.4971\n",
      "Epoch 110/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1213667757.4408 - val_loss: 757152341.5205\n",
      "Epoch 111/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1262101134.4395 - val_loss: 758359876.9903\n",
      "Epoch 112/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1278266427.7555 - val_loss: 777044538.0741\n",
      "Epoch 113/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1236014782.0026 - val_loss: 890604001.8090\n",
      "Epoch 114/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1345683631.9376 - val_loss: 1003184926.1910\n",
      "Epoch 115/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1483849572.1196 - val_loss: 1053353979.7583\n",
      "Epoch 116/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1450812554.0702 - val_loss: 923665523.0253\n",
      "Epoch 117/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1312687602.8505 - val_loss: 837990309.1774\n",
      "Epoch 118/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1274434103.1782 - val_loss: 849989892.4912\n",
      "Epoch 119/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1271957511.6567 - val_loss: 840371352.3899\n",
      "Epoch 120/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1278600676.3693 - val_loss: 820564211.3996\n",
      "Epoch 121/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1235089066.7776 - val_loss: 891925839.8441\n",
      "Epoch 122/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1356787682.4551 - val_loss: 1182148456.7953\n",
      "Epoch 123/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1578992917.3056 - val_loss: 1109672581.7388\n",
      "Epoch 124/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1484340291.2458 - val_loss: 826276198.7992\n",
      "Epoch 125/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1315582956.6710 - val_loss: 946161389.5049\n",
      "Epoch 126/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1592117305.5345 - val_loss: 977433819.1969\n",
      "Epoch 127/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1466441080.3225 - val_loss: 825445829.9259\n",
      "Epoch 128/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1265419183.9376 - val_loss: 797454298.3236\n",
      "Epoch 129/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1288801431.2198 - val_loss: 817376124.1326\n",
      "Epoch 130/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1317124200.9467 - val_loss: 855131531.7271\n",
      "Epoch 131/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1349895471.6047 - val_loss: 882175023.6569\n",
      "Epoch 132/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1196365387.5527 - val_loss: 802003603.3996\n",
      "Epoch 133/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 63us/step - loss: 1357112062.3355 - val_loss: 883795624.0468\n",
      "Epoch 134/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1381798881.7061 - val_loss: 982186969.2632\n",
      "Epoch 135/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1651979010.1638 - val_loss: 1235593526.8498\n",
      "Epoch 136/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1535944003.7451 - val_loss: 862195254.3314\n",
      "Epoch 137/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1375053444.1612 - val_loss: 948047598.5341\n",
      "Epoch 138/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1375892065.8726 - val_loss: 938859728.3431\n",
      "Epoch 139/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1370776938.9025 - val_loss: 921700304.5926\n",
      "Epoch 140/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1343085640.9883 - val_loss: 912960735.0643\n",
      "Epoch 141/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1391830095.4070 - val_loss: 838575529.0448\n",
      "Epoch 142/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1303018433.3732 - val_loss: 805394808.0156\n",
      "Epoch 143/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1396511112.8218 - val_loss: 798515970.6199\n",
      "Epoch 144/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1223481928.2393 - val_loss: 782807249.0916\n",
      "Epoch 145/1000\n",
      "1538/1538 [==============================] - 0s 55us/step - loss: 1304379618.1222 - val_loss: 779510086.4873\n",
      "Epoch 146/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1285112013.7321 - val_loss: 798532087.0175\n",
      "Epoch 147/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1203790922.2367 - val_loss: 813615464.7953\n",
      "Epoch 148/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1215807929.3420 - val_loss: 794297395.3996\n",
      "Epoch 149/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1218274191.9792 - val_loss: 758603196.3821\n",
      "Epoch 150/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1189807503.8127 - val_loss: 754054469.1774\n",
      "Epoch 151/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1207102226.1430 - val_loss: 753072887.7661\n",
      "Epoch 152/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1260639817.0715 - val_loss: 744410255.7193\n",
      "Epoch 153/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1231357577.2380 - val_loss: 737901245.8791\n",
      "Epoch 154/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1245895718.2835 - val_loss: 735755407.9688\n",
      "Epoch 155/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1219912873.5969 - val_loss: 739165560.6394\n",
      "Epoch 156/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1158146459.4642 - val_loss: 754618017.9337\n",
      "Epoch 157/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1280310709.4720 - val_loss: 916134686.9396\n",
      "Epoch 158/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1363348747.0481 - val_loss: 913293827.2437\n",
      "Epoch 159/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1282331218.3927 - val_loss: 783746830.9708\n",
      "Epoch 160/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1245319897.1339 - val_loss: 757080449.4971\n",
      "Epoch 161/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1191080315.0065 - val_loss: 730374306.1209\n",
      "Epoch 162/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1286215076.8687 - val_loss: 724122909.0682\n",
      "Epoch 163/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1234626230.2627 - val_loss: 730295520.0624\n",
      "Epoch 164/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1187358956.6086 - val_loss: 739912469.2398\n",
      "Epoch 165/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1266941543.0325 - val_loss: 720151039.7505\n",
      "Epoch 166/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1241259659.4434 - val_loss: 867509613.7856\n",
      "Epoch 167/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1265670909.3368 - val_loss: 899942515.7739\n",
      "Epoch 168/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1396701358.2731 - val_loss: 744541408.3119\n",
      "Epoch 169/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1270148091.2146 - val_loss: 777713064.4834\n",
      "Epoch 170/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1322754067.3082 - val_loss: 751582293.6452\n",
      "Epoch 171/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1195517928.9051 - val_loss: 752857360.7173\n",
      "Epoch 172/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1250331003.1730 - val_loss: 767814040.3275\n",
      "Epoch 173/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1220684487.8856 - val_loss: 711236505.9493\n",
      "Epoch 174/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1198117816.1769 - val_loss: 730221486.1598\n",
      "Epoch 175/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1153936024.7178 - val_loss: 778490380.4133\n",
      "Epoch 176/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1284368848.4785 - val_loss: 716942312.1715\n",
      "Epoch 177/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1229586996.5566 - val_loss: 733387800.4522\n",
      "Epoch 178/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1161300805.5761 - val_loss: 790118974.1287\n",
      "Epoch 179/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1221671041.2068 - val_loss: 816905122.0585\n",
      "Epoch 180/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1238281208.6762 - val_loss: 691129215.4386\n",
      "Epoch 181/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1252142886.1170 - val_loss: 833592406.5644\n",
      "Epoch 182/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1395973170.2679 - val_loss: 751727426.7329\n",
      "Epoch 183/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1183938852.3069 - val_loss: 760202931.5244\n",
      "Epoch 184/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1222748932.4941 - val_loss: 964892097.9961\n",
      "Epoch 185/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1348900690.4759 - val_loss: 1067065529.6374\n",
      "Epoch 186/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1413717113.6749 - val_loss: 792222615.2047\n",
      "Epoch 187/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1139432416.7334 - val_loss: 752433163.1326\n",
      "Epoch 188/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1255086176.4785 - val_loss: 777414889.2437\n",
      "Epoch 189/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1240390062.1066 - val_loss: 826329365.2086\n",
      "Epoch 190/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1496063360.8322 - val_loss: 1168298793.9181\n",
      "Epoch 191/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1296825176.8843 - val_loss: 995525222.7415\n",
      "Epoch 192/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1797901365.8466 - val_loss: 1601599543.9220\n",
      "Epoch 193/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 2104445972.6398 - val_loss: 827532006.1754\n",
      "Epoch 194/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1391764092.3381 - val_loss: 2319727192.8265\n",
      "Epoch 195/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 2737384488.2185 - val_loss: 2922006326.3938\n",
      "Epoch 196/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 2767943317.1391 - val_loss: 1707636884.7096\n",
      "Epoch 197/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1612440503.8440 - val_loss: 758971953.8402\n",
      "Epoch 198/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1336157259.7763 - val_loss: 1279276121.8869\n",
      "Epoch 199/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 62us/step - loss: 1894248880.1040 - val_loss: 1016472229.0526\n",
      "Epoch 200/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1364148197.0767 - val_loss: 907182569.7934\n",
      "Epoch 201/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1310164062.0338 - val_loss: 1341219548.0702\n",
      "Epoch 202/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1759313459.6203 - val_loss: 1193458667.0409\n",
      "Epoch 203/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1412500694.3875 - val_loss: 770869229.1618\n",
      "Epoch 204/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1530545726.4187 - val_loss: 2422159689.1072\n",
      "Epoch 205/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 3176376648.4057 - val_loss: 2313899124.7719\n",
      "Epoch 206/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 2288383323.2146 - val_loss: 833053138.2768\n",
      "Epoch 207/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1185566503.3030 - val_loss: 1192113101.5984\n",
      "Epoch 208/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1715201427.1417 - val_loss: 1494903009.5595\n",
      "Epoch 209/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1696907471.7295 - val_loss: 971295453.3177\n",
      "Epoch 210/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1263044820.0572 - val_loss: 810596717.9727\n",
      "Epoch 211/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1351778878.3875 - val_loss: 1007462415.0955\n",
      "Epoch 212/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1511583387.5475 - val_loss: 851775732.8967\n",
      "Epoch 213/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1207263244.1508 - val_loss: 834531228.1949\n",
      "Epoch 214/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1200779805.9610 - val_loss: 813430847.2515\n",
      "Epoch 215/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1275530090.8609 - val_loss: 979218263.6803\n",
      "Epoch 216/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1430817333.0975 - val_loss: 1088450370.1362\n",
      "Epoch 217/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1414262313.4460 - val_loss: 866214641.6764\n",
      "Epoch 218/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1202552777.9870 - val_loss: 778187219.5244\n",
      "Epoch 219/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1199994712.2185 - val_loss: 799932927.6257\n",
      "Epoch 220/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1105666296.6762 - val_loss: 801551202.3080\n",
      "Epoch 221/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1243860424.5722 - val_loss: 787692014.2846\n",
      "Epoch 222/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1126403007.4902 - val_loss: 801161050.1988\n",
      "Epoch 223/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1242578254.2315 - val_loss: 787302798.0975\n",
      "Epoch 224/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1307989619.3914 - val_loss: 789035743.9376\n",
      "Epoch 225/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1171241627.9116 - val_loss: 765109340.8811\n",
      "Epoch 226/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1184104526.6060 - val_loss: 742292846.0975\n",
      "Epoch 227/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1171158579.4330 - val_loss: 745549561.4503\n",
      "Epoch 228/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1240680131.9116 - val_loss: 756695645.0760\n",
      "Epoch 229/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1156464136.4889 - val_loss: 744823772.2261\n",
      "Epoch 230/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1182795219.9324 - val_loss: 739786093.3801\n",
      "Epoch 231/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1107842348.5670 - val_loss: 779121122.8070\n",
      "Epoch 232/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1201673112.6313 - val_loss: 796634375.4854\n",
      "Epoch 233/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1192074072.1769 - val_loss: 761795454.6277\n",
      "Epoch 234/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1125614227.5579 - val_loss: 737556676.1793\n",
      "Epoch 235/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1185742096.6996 - val_loss: 723131375.7817\n",
      "Epoch 236/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1111173874.0182 - val_loss: 719058703.3138\n",
      "Epoch 237/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1227728105.2796 - val_loss: 759684333.2242\n",
      "Epoch 238/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1144051976.8218 - val_loss: 801356078.4094\n",
      "Epoch 239/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1263091416.4681 - val_loss: 939806675.7115\n",
      "Epoch 240/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1263449548.8999 - val_loss: 837790877.9415\n",
      "Epoch 241/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1188375001.2172 - val_loss: 681149911.9532\n",
      "Epoch 242/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1106969566.3771 - val_loss: 707377938.9474\n",
      "Epoch 243/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1194048791.6359 - val_loss: 779403575.4281\n",
      "Epoch 244/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1220094231.4798 - val_loss: 874092405.1852\n",
      "Epoch 245/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1328966758.0754 - val_loss: 777945182.0936\n",
      "Epoch 246/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1176008623.2718 - val_loss: 776324129.3411\n",
      "Epoch 247/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1149592903.1573 - val_loss: 808370083.4932\n",
      "Epoch 248/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1209541913.2172 - val_loss: 796723483.1969\n",
      "Epoch 249/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1245528743.1157 - val_loss: 821976335.3450\n",
      "Epoch 250/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1453785671.2406 - val_loss: 1282523167.4386\n",
      "Epoch 251/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1481092873.4876 - val_loss: 984512610.5575\n",
      "Epoch 252/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1223845401.7529 - val_loss: 763544845.0994\n",
      "Epoch 253/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1158031425.6645 - val_loss: 793260813.7388\n",
      "Epoch 254/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1154369377.7893 - val_loss: 745509943.1423\n",
      "Epoch 255/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1220472868.9519 - val_loss: 699344166.7368\n",
      "Epoch 256/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1233947008.0832 - val_loss: 677121974.8304\n",
      "Epoch 257/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1062626979.7035 - val_loss: 714974556.8187\n",
      "Epoch 258/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1167085246.0858 - val_loss: 811098336.4366\n",
      "Epoch 259/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1375446435.3914 - val_loss: 779534488.7018\n",
      "Epoch 260/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1188720169.8622 - val_loss: 698274588.4444\n",
      "Epoch 261/1000\n",
      "1538/1538 [==============================] - ETA: 0s - loss: 1079095082.66 - 0s 64us/step - loss: 1079296427.6099 - val_loss: 684235038.1287\n",
      "Epoch 262/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1117668507.1730 - val_loss: 682801088.4366\n",
      "Epoch 263/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1158750368.2913 - val_loss: 688899226.6355\n",
      "Epoch 264/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1094667983.2302 - val_loss: 708911920.1559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1075657995.4850 - val_loss: 698914102.3314\n",
      "Epoch 266/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1072485101.6905 - val_loss: 717433436.5068\n",
      "Epoch 267/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1148410172.3485 - val_loss: 748647779.9922\n",
      "Epoch 268/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1095374536.2809 - val_loss: 723998814.2846\n",
      "Epoch 269/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1126254741.0975 - val_loss: 729022786.4951\n",
      "Epoch 270/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1210746762.4031 - val_loss: 740745742.4717\n",
      "Epoch 271/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1098028921.1756 - val_loss: 740983985.6530\n",
      "Epoch 272/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1070879928.7594 - val_loss: 790205463.5789\n",
      "Epoch 273/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1231205110.6372 - val_loss: 789863361.9961\n",
      "Epoch 274/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1189970552.8427 - val_loss: 694106025.3255\n",
      "Epoch 275/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1171798529.5813 - val_loss: 680320832.1150\n",
      "Epoch 276/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1116617373.5449 - val_loss: 681221111.5288\n",
      "Epoch 277/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1126538053.9298 - val_loss: 712634947.6647\n",
      "Epoch 278/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1186528273.6853 - val_loss: 763224108.6316\n",
      "Epoch 279/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1194065628.0468 - val_loss: 734077819.9454\n",
      "Epoch 280/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1140792736.7906 - val_loss: 689714145.1540\n",
      "Epoch 281/1000\n",
      "1538/1538 [==============================] - 0s 81us/step - loss: 1142120581.3264 - val_loss: 696900112.1404\n",
      "Epoch 282/1000\n",
      "1538/1538 [==============================] - 0s 83us/step - loss: 1082822475.6723 - val_loss: 706072701.3645\n",
      "Epoch 283/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1122253288.1873 - val_loss: 717197018.1988\n",
      "Epoch 284/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1068161690.3823 - val_loss: 713901781.0838\n",
      "Epoch 285/1000\n",
      "1538/1538 [==============================] - 0s 89us/step - loss: 1046361887.6255 - val_loss: 709701192.6394\n",
      "Epoch 286/1000\n",
      "1538/1538 [==============================] - 0s 117us/step - loss: 1094609248.8114 - val_loss: 704736376.3431\n",
      "Epoch 287/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1179306079.3758 - val_loss: 728452137.8928\n",
      "Epoch 288/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1180713863.1131 - val_loss: 763113579.9529\n",
      "Epoch 289/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1149583226.1743 - val_loss: 727707220.3938\n",
      "Epoch 290/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1120210800.5826 - val_loss: 710618465.6998\n",
      "Epoch 291/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1079047732.0468 - val_loss: 717632556.6628\n",
      "Epoch 292/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1132276627.8075 - val_loss: 718341763.8674\n",
      "Epoch 293/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1057987370.4447 - val_loss: 731097921.4035\n",
      "Epoch 294/1000\n",
      "1538/1538 [==============================] - 0s 79us/step - loss: 1184068314.2991 - val_loss: 737509686.6745\n",
      "Epoch 295/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1149427773.6697 - val_loss: 707816658.3704\n",
      "Epoch 296/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1074675975.6567 - val_loss: 706494941.4425\n",
      "Epoch 297/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1067934645.0975 - val_loss: 866067868.6940\n",
      "Epoch 298/1000\n",
      "1538/1538 [==============================] - 0s 55us/step - loss: 1212426058.4031 - val_loss: 894857869.4737\n",
      "Epoch 299/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1226295534.1899 - val_loss: 729468213.5828\n",
      "Epoch 300/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1244300510.4187 - val_loss: 698077726.3782\n",
      "Epoch 301/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1158205876.8895 - val_loss: 691990324.3041\n",
      "Epoch 302/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1162631376.8947 - val_loss: 676276715.7895\n",
      "Epoch 303/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1100919414.5540 - val_loss: 703559426.1684\n",
      "Epoch 304/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1158419382.9285 - val_loss: 667763659.0487\n",
      "Epoch 305/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1110336875.7347 - val_loss: 700537935.1579\n",
      "Epoch 306/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1157583706.0494 - val_loss: 693417894.6121\n",
      "Epoch 307/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1075019200.0832 - val_loss: 721944649.7310\n",
      "Epoch 308/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1159754966.9285 - val_loss: 1092399856.5302\n",
      "Epoch 309/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1488449366.4291 - val_loss: 1052611263.0019\n",
      "Epoch 310/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1290612289.7477 - val_loss: 813005912.1404\n",
      "Epoch 311/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1168674708.1404 - val_loss: 793602914.3626\n",
      "Epoch 312/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1237255079.4486 - val_loss: 794972175.3138\n",
      "Epoch 313/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1259255316.8062 - val_loss: 1026206221.8480\n",
      "Epoch 314/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1283392898.1430 - val_loss: 876751375.4698\n",
      "Epoch 315/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1234582709.4304 - val_loss: 772910887.4542\n",
      "Epoch 316/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1143291049.6957 - val_loss: 814623923.5543\n",
      "Epoch 317/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1164521449.8622 - val_loss: 810381854.4459\n",
      "Epoch 318/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1065683486.6268 - val_loss: 823215857.8402\n",
      "Epoch 319/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1240642020.3693 - val_loss: 904923561.7934\n",
      "Epoch 320/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1156115929.5501 - val_loss: 867542491.6959\n",
      "Epoch 321/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1137073908.2705 - val_loss: 812362811.1345\n",
      "Epoch 322/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1153652885.8049 - val_loss: 793546112.9045\n",
      "Epoch 323/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1134356729.2588 - val_loss: 860600047.6569\n",
      "Epoch 324/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1261808136.1144 - val_loss: 845986392.5380\n",
      "Epoch 325/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1213520262.4915 - val_loss: 783110243.9298\n",
      "Epoch 326/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1153546121.7581 - val_loss: 758112937.6296\n",
      "Epoch 327/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 996699631.1365 - val_loss: 732206351.9454\n",
      "Epoch 328/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1150906987.0273 - val_loss: 720659122.7290\n",
      "Epoch 329/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1123226063.3966 - val_loss: 738657864.4211\n",
      "Epoch 330/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1170549621.8466 - val_loss: 966854571.9142\n",
      "Epoch 331/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 57us/step - loss: 1246681842.3511 - val_loss: 912937231.7193\n",
      "Epoch 332/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1250126191.1886 - val_loss: 781095547.6335\n",
      "Epoch 333/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1223972818.7256 - val_loss: 893606710.6433\n",
      "Epoch 334/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1304980309.7217 - val_loss: 1692869331.5867\n",
      "Epoch 335/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1944358112.7074 - val_loss: 1297776772.2417\n",
      "Epoch 336/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1467841504.4785 - val_loss: 783286573.9103\n",
      "Epoch 337/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1114352337.8934 - val_loss: 814932085.2125\n",
      "Epoch 338/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1239525810.4759 - val_loss: 824268315.3762\n",
      "Epoch 339/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1346154032.1873 - val_loss: 731496292.0312\n",
      "Epoch 340/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1157760826.3719 - val_loss: 777938599.2359\n",
      "Epoch 341/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1208748405.5137 - val_loss: 728226293.2710\n",
      "Epoch 342/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1090145338.9233 - val_loss: 992845939.3060\n",
      "Epoch 343/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1430922423.5735 - val_loss: 914685245.1111\n",
      "Epoch 344/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1192907008.0000 - val_loss: 755047729.3411\n",
      "Epoch 345/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1188631036.2003 - val_loss: 1373188476.2573\n",
      "Epoch 346/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1734866623.5839 - val_loss: 1354568180.2729\n",
      "Epoch 347/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1480024031.0429 - val_loss: 836832829.0058\n",
      "Epoch 348/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1069892951.5527 - val_loss: 795493553.0916\n",
      "Epoch 349/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1147169078.2627 - val_loss: 781240915.6491\n",
      "Epoch 350/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1297291937.0611 - val_loss: 1482150915.4932\n",
      "Epoch 351/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1942551606.5956 - val_loss: 1012996750.2222\n",
      "Epoch 352/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1177341165.9610 - val_loss: 1235250212.9279\n",
      "Epoch 353/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 2018589566.0858 - val_loss: 1624111164.6316\n",
      "Epoch 354/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1940656150.4083 - val_loss: 843171019.1813\n",
      "Epoch 355/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1285815991.8440 - val_loss: 893909021.5673\n",
      "Epoch 356/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1400015128.1118 - val_loss: 1070213332.5848\n",
      "Epoch 357/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1496619157.2224 - val_loss: 894275060.3977\n",
      "Epoch 358/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1223503610.6736 - val_loss: 765824985.9805\n",
      "Epoch 359/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1095894095.3966 - val_loss: 769888555.5088\n",
      "Epoch 360/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1118326375.3654 - val_loss: 753794034.9513\n",
      "Epoch 361/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1233777768.8635 - val_loss: 799851735.3294\n",
      "Epoch 362/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1356414298.1743 - val_loss: 812840040.7329\n",
      "Epoch 363/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1190152666.6320 - val_loss: 738586883.0565\n",
      "Epoch 364/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1114466977.9038 - val_loss: 774608858.3080\n",
      "Epoch 365/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1090172937.4876 - val_loss: 844194638.9084\n",
      "Epoch 366/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1164751347.5371 - val_loss: 778222952.5614\n",
      "Epoch 367/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1175312493.8570 - val_loss: 745878944.9201\n",
      "Epoch 368/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1064913889.1651 - val_loss: 779794377.4191\n",
      "Epoch 369/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1275791333.4512 - val_loss: 798164161.4035\n",
      "Epoch 370/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1161367081.6125 - val_loss: 751580618.1832\n",
      "Epoch 371/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1174675903.4174 - val_loss: 678699672.8967\n",
      "Epoch 372/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1009405087.2302 - val_loss: 750951571.9610\n",
      "Epoch 373/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1178940679.6567 - val_loss: 752802578.6823\n",
      "Epoch 374/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1148779581.9402 - val_loss: 695245419.8207\n",
      "Epoch 375/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1041700905.2796 - val_loss: 649627458.8470\n",
      "Epoch 376/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1046800424.7282 - val_loss: 651090878.7057\n",
      "Epoch 377/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1103381170.3927 - val_loss: 656509375.6413\n",
      "Epoch 378/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 964071822.8973 - val_loss: 654102606.4639\n",
      "Epoch 379/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1074794531.6203 - val_loss: 666176948.4181\n",
      "Epoch 380/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1093424614.2003 - val_loss: 687753096.0515\n",
      "Epoch 381/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1104849506.5384 - val_loss: 670619053.4035\n",
      "Epoch 382/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1121343760.5618 - val_loss: 699864539.7895\n",
      "Epoch 383/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1126136282.4239 - val_loss: 686615191.6881\n",
      "Epoch 384/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1031533959.1573 - val_loss: 680711645.6608\n",
      "Epoch 385/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1104025329.7425 - val_loss: 743922141.4425\n",
      "Epoch 386/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1142071362.0806 - val_loss: 720863527.4854\n",
      "Epoch 387/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1027549959.7191 - val_loss: 673634372.3821\n",
      "Epoch 388/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1080697609.1964 - val_loss: 662731038.2612\n",
      "Epoch 389/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1057028020.5566 - val_loss: 686472099.7290\n",
      "Epoch 390/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1075551638.8036 - val_loss: 660197392.2495\n",
      "Epoch 391/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1089228230.3251 - val_loss: 929078137.5127\n",
      "Epoch 392/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1428048173.0247 - val_loss: 993774412.8499\n",
      "Epoch 393/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1114662894.7308 - val_loss: 759155185.8402\n",
      "Epoch 394/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1076306379.9012 - val_loss: 701302794.9786\n",
      "Epoch 395/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1040769768.1977 - val_loss: 713424572.5068\n",
      "Epoch 396/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1077896375.7607 - val_loss: 700419102.2222\n",
      "Epoch 397/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 58us/step - loss: 1132407643.5475 - val_loss: 1140548741.3177\n",
      "Epoch 398/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1575496867.0377 - val_loss: 1202206907.2593\n",
      "Epoch 399/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1456782721.6645 - val_loss: 751231601.3567\n",
      "Epoch 400/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1106964592.1665 - val_loss: 909982657.2476\n",
      "Epoch 401/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1324212005.1183 - val_loss: 1011848113.9025\n",
      "Epoch 402/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1356756402.1014 - val_loss: 831635533.8480\n",
      "Epoch 403/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1059735669.0247 - val_loss: 747672646.4873\n",
      "Epoch 404/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1055651904.5410 - val_loss: 765284622.3626\n",
      "Epoch 405/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1076564087.8752 - val_loss: 781319595.3840\n",
      "Epoch 406/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1126975043.2250 - val_loss: 753624684.1637\n",
      "Epoch 407/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1060289081.2588 - val_loss: 957432543.5634\n",
      "Epoch 408/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1469835058.1014 - val_loss: 1823367211.9142\n",
      "Epoch 409/1000\n",
      "1538/1538 [==============================] - 0s 55us/step - loss: 1993426410.6944 - val_loss: 1397873927.9844\n",
      "Epoch 410/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1454199834.2991 - val_loss: 862077688.5770\n",
      "Epoch 411/1000\n",
      "1538/1538 [==============================] - 0s 85us/step - loss: 1121574469.0767 - val_loss: 952529253.3894\n",
      "Epoch 412/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1391868067.6203 - val_loss: 964941760.3236\n",
      "Epoch 413/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1349169517.1912 - val_loss: 785665565.0370\n",
      "Epoch 414/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1043135234.2211 - val_loss: 1118950117.6764\n",
      "Epoch 415/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1417640301.7321 - val_loss: 1133008219.9454\n",
      "Epoch 416/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1351560700.0468 - val_loss: 871480088.3899\n",
      "Epoch 417/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1194369950.0442 - val_loss: 747354960.1192\n",
      "Epoch 418/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1117142590.6684 - val_loss: 779911838.6979\n",
      "Epoch 419/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1108472000.4161 - val_loss: 732313009.5192\n",
      "Epoch 420/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1044442976.7282 - val_loss: 718520799.2164\n",
      "Epoch 421/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1031328801.6229 - val_loss: 787357589.5205\n",
      "Epoch 422/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1232519982.9389 - val_loss: 1069156777.1696\n",
      "Epoch 423/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1373453355.9428 - val_loss: 1048074232.6394\n",
      "Epoch 424/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1309529819.7139 - val_loss: 773381963.6023\n",
      "Epoch 425/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1128213108.9727 - val_loss: 789592277.3865\n",
      "Epoch 426/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1083011419.8804 - val_loss: 800813352.2895\n",
      "Epoch 427/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1134361586.3511 - val_loss: 871876467.8986\n",
      "Epoch 428/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1212713313.3108 - val_loss: 865932943.9064\n",
      "Epoch 429/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1203854312.5618 - val_loss: 783691926.8616\n",
      "Epoch 430/1000\n",
      "1538/1538 [==============================] - 0s 90us/step - loss: 1043734254.5644 - val_loss: 736362126.4620\n",
      "Epoch 431/1000\n",
      "1538/1538 [==============================] - 0s 233us/step - loss: 1144065278.9181 - val_loss: 725576805.0361\n",
      "Epoch 432/1000\n",
      "1538/1538 [==============================] - 0s 303us/step - loss: 1129583355.0897 - val_loss: 718698742.7690\n",
      "Epoch 433/1000\n",
      "1538/1538 [==============================] - 0s 86us/step - loss: 1044737720.4265 - val_loss: 704841981.0074\n",
      "Epoch 434/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1202557500.2055 - val_loss: 881935819.4152\n",
      "Epoch 435/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1466525842.0182 - val_loss: 819765539.7739\n",
      "Epoch 436/1000\n",
      "1538/1538 [==============================] - 0s 83us/step - loss: 1160354568.1560 - val_loss: 714577753.8431\n",
      "Epoch 437/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1092243849.8205 - val_loss: 772377613.6296\n",
      "Epoch 438/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1125714009.9662 - val_loss: 956026455.7037\n",
      "Epoch 439/1000\n",
      "1538/1538 [==============================] - 0s 86us/step - loss: 1308707060.1821 - val_loss: 848412268.7875\n",
      "Epoch 440/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 1116336693.4512 - val_loss: 709272831.5945\n",
      "Epoch 441/1000\n",
      "1538/1538 [==============================] - 0s 91us/step - loss: 997724165.5761 - val_loss: 701196413.0448\n",
      "Epoch 442/1000\n",
      "1538/1538 [==============================] - 0s 134us/step - loss: 1025665788.7542 - val_loss: 757830670.6541\n",
      "Epoch 443/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1153232845.1495 - val_loss: 719824284.3713\n",
      "Epoch 444/1000\n",
      "1538/1538 [==============================] - 0s 99us/step - loss: 1115974033.9766 - val_loss: 734821613.6920\n",
      "Epoch 445/1000\n",
      "1538/1538 [==============================] - 0s 120us/step - loss: 1116084228.9935 - val_loss: 914078117.1774\n",
      "Epoch 446/1000\n",
      "1538/1538 [==============================] - 0s 138us/step - loss: 1337246793.5709 - val_loss: 855986004.3353\n",
      "Epoch 447/1000\n",
      "1538/1538 [==============================] - 0s 156us/step - loss: 1140238406.4915 - val_loss: 727361045.7232\n",
      "Epoch 448/1000\n",
      "1538/1538 [==============================] - 0s 99us/step - loss: 1102328943.1886 - val_loss: 842754309.7310\n",
      "Epoch 449/1000\n",
      "1538/1538 [==============================] - 0s 115us/step - loss: 1113561127.4486 - val_loss: 725746381.9727\n",
      "Epoch 450/1000\n",
      "1538/1538 [==============================] - 0s 156us/step - loss: 1086597928.9883 - val_loss: 845560249.7622\n",
      "Epoch 451/1000\n",
      "1538/1538 [==============================] - 0s 129us/step - loss: 1145233644.1925 - val_loss: 908133261.3489\n",
      "Epoch 452/1000\n",
      "1538/1538 [==============================] - 0s 90us/step - loss: 1112087858.5176 - val_loss: 783430711.1423\n",
      "Epoch 453/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1067257600.5410 - val_loss: 712279997.7544\n",
      "Epoch 454/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1022361709.1079 - val_loss: 705319709.1306\n",
      "Epoch 455/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 998609805.0247 - val_loss: 697686031.1891\n",
      "Epoch 456/1000\n",
      "1538/1538 [==============================] - 0s 82us/step - loss: 1094061066.3199 - val_loss: 714800815.7271\n",
      "Epoch 457/1000\n",
      "1538/1538 [==============================] - 0s 149us/step - loss: 1060013843.4746 - val_loss: 701360355.9025\n",
      "Epoch 458/1000\n",
      "1538/1538 [==============================] - 0s 154us/step - loss: 1044097380.6918 - val_loss: 702912290.0897\n",
      "Epoch 459/1000\n",
      "1538/1538 [==============================] - 0s 80us/step - loss: 1002456916.5566 - val_loss: 759073559.5166\n",
      "Epoch 460/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 1062345253.2848 - val_loss: 794601132.4133\n",
      "Epoch 461/1000\n",
      "1538/1538 [==============================] - 0s 82us/step - loss: 1047295428.0780 - val_loss: 722057340.6004\n",
      "Epoch 462/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 963204978.8505 - val_loss: 745821503.7817\n",
      "Epoch 463/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 75us/step - loss: 1059930286.8140 - val_loss: 810872011.9766\n",
      "Epoch 464/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1128610624.2497 - val_loss: 804507984.3743\n",
      "Epoch 465/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1103572321.7893 - val_loss: 828947894.1442\n",
      "Epoch 466/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1112436196.7022 - val_loss: 852457219.6179\n",
      "Epoch 467/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1142431267.2978 - val_loss: 793154832.1559\n",
      "Epoch 468/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1094636493.4824 - val_loss: 803347798.1442\n",
      "Epoch 469/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1070262392.8427 - val_loss: 783439943.2359\n",
      "Epoch 470/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1230152542.0442 - val_loss: 943239680.6238\n",
      "Epoch 471/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1323887568.5618 - val_loss: 1002063960.5770\n",
      "Epoch 472/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1167688452.6190 - val_loss: 822086640.0312\n",
      "Epoch 473/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1065275873.3004 - val_loss: 757528728.7953\n",
      "Epoch 474/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1140284776.6138 - val_loss: 770033863.0097\n",
      "Epoch 475/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1088521377.8934 - val_loss: 754989883.6491\n",
      "Epoch 476/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 982621202.9753 - val_loss: 774984498.6511\n",
      "Epoch 477/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 992613541.1183 - val_loss: 853035143.0487\n",
      "Epoch 478/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1222955521.1651 - val_loss: 1069606074.5107\n",
      "Epoch 479/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1324740581.9506 - val_loss: 883072550.1131\n",
      "Epoch 480/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1125772433.8934 - val_loss: 728834335.2749\n",
      "Epoch 481/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1072855942.7828 - val_loss: 767794159.7583\n",
      "Epoch 482/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1076926970.4239 - val_loss: 760198790.7368\n",
      "Epoch 483/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1094801010.3927 - val_loss: 735778790.1754\n",
      "Epoch 484/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1094477117.5865 - val_loss: 756831175.5789\n",
      "Epoch 485/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1063704271.7295 - val_loss: 738143862.5848\n",
      "Epoch 486/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1077035482.0494 - val_loss: 1292931762.8382\n",
      "Epoch 487/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 2215230739.0377 - val_loss: 2740919856.6550\n",
      "Epoch 488/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 2900371902.0858 - val_loss: 843286642.2456\n",
      "Epoch 489/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1712976823.4278 - val_loss: 3693564221.3801\n",
      "Epoch 490/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 4345912834.4967 - val_loss: 4413149276.8187\n",
      "Epoch 491/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 4132650051.5787 - val_loss: 2595010518.0819\n",
      "Epoch 492/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 2210769604.4941 - val_loss: 1100552662.3314\n",
      "Epoch 493/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1412474569.8205 - val_loss: 1562801732.3977\n",
      "Epoch 494/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 2156903051.3186 - val_loss: 1501773698.0819\n",
      "Epoch 495/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1555782768.3537 - val_loss: 981772717.7232\n",
      "Epoch 496/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1296513614.8973 - val_loss: 1260561748.5848\n",
      "Epoch 497/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1539750218.2367 - val_loss: 1068696195.7427\n",
      "Epoch 498/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1312075517.2120 - val_loss: 840585845.0804\n",
      "Epoch 499/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1266902806.5735 - val_loss: 929745427.4620\n",
      "Epoch 500/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1405357527.0533 - val_loss: 874519960.7641\n",
      "Epoch 501/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1199995330.4759 - val_loss: 775664877.5517\n",
      "Epoch 502/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1088865080.2185 - val_loss: 842452432.8553\n",
      "Epoch 503/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1197777679.2562 - val_loss: 873530144.0585\n",
      "Epoch 504/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1204702901.7633 - val_loss: 780369682.0136\n",
      "Epoch 505/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1181752178.3095 - val_loss: 764887710.9396\n",
      "Epoch 506/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1078678992.0624 - val_loss: 821869982.7836\n",
      "Epoch 507/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1112071831.5527 - val_loss: 775030247.1423\n",
      "Epoch 508/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1088774756.1560 - val_loss: 772880317.3499\n",
      "Epoch 509/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1038715452.1196 - val_loss: 798127494.9942\n",
      "Epoch 510/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1036651532.8999 - val_loss: 800880974.9961\n",
      "Epoch 511/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1119269451.4850 - val_loss: 764076795.5400\n",
      "Epoch 512/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1076254000.1873 - val_loss: 861471632.6550\n",
      "Epoch 513/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1197162558.5436 - val_loss: 829570454.3626\n",
      "Epoch 514/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1114371168.8739 - val_loss: 761464877.6425\n",
      "Epoch 515/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1027408449.9558 - val_loss: 880047525.9883\n",
      "Epoch 516/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1139692346.8817 - val_loss: 894298121.8246\n",
      "Epoch 517/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1136580472.2081 - val_loss: 814093302.1287\n",
      "Epoch 518/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1029301249.7477 - val_loss: 747613060.1886\n",
      "Epoch 519/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1092692848.2705 - val_loss: 754562829.7115\n",
      "Epoch 520/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1053600444.5878 - val_loss: 729708041.6467\n",
      "Epoch 521/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1063715986.3095 - val_loss: 725976338.8918\n",
      "Epoch 522/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 990897665.6645 - val_loss: 710855071.0117\n",
      "Epoch 523/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1063679457.8726 - val_loss: 748226979.5556\n",
      "Epoch 524/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1157352928.9571 - val_loss: 745921323.0565\n",
      "Epoch 525/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1034404699.2146 - val_loss: 695258611.5429\n",
      "Epoch 526/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1061298477.5553 - val_loss: 777100957.9727\n",
      "Epoch 527/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1080526570.5280 - val_loss: 842202877.2242\n",
      "Epoch 528/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1193592349.0559 - val_loss: 737145810.5107\n",
      "Epoch 529/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 66us/step - loss: 1035368836.2445 - val_loss: 708370717.1793\n",
      "Epoch 530/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1120682118.0754 - val_loss: 762834236.4522\n",
      "Epoch 531/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1084894686.5436 - val_loss: 758531334.3470\n",
      "Epoch 532/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1134664744.2002 - val_loss: 739448320.6949\n",
      "Epoch 533/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1038438178.7880 - val_loss: 728032164.7407\n",
      "Epoch 534/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1064838693.9922 - val_loss: 731489607.1379\n",
      "Epoch 535/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1050681347.9948 - val_loss: 760773018.8548\n",
      "Epoch 536/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1065553432.5098 - val_loss: 937452068.0078\n",
      "Epoch 537/1000\n",
      "1538/1538 [==============================] - 0s 55us/step - loss: 1238059684.2861 - val_loss: 812813294.2027\n",
      "Epoch 538/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1086243703.5111 - val_loss: 778276658.5887\n",
      "Epoch 539/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1055804894.0494 - val_loss: 781541217.3411\n",
      "Epoch 540/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1041309006.2315 - val_loss: 956554552.4522\n",
      "Epoch 541/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1554364740.5774 - val_loss: 2195952040.6706\n",
      "Epoch 542/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 2424603265.8309 - val_loss: 1563497723.2593\n",
      "Epoch 543/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1420427805.2952 - val_loss: 871974756.6082\n",
      "Epoch 544/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1268486489.9662 - val_loss: 1329733794.9942\n",
      "Epoch 545/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1529251967.9168 - val_loss: 1039219140.5224\n",
      "Epoch 546/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1314173188.0780 - val_loss: 880523163.6959\n",
      "Epoch 547/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1236125704.4057 - val_loss: 1009031294.6901\n",
      "Epoch 548/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1219113916.0884 - val_loss: 1009900259.8051\n",
      "Epoch 549/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1235605967.8960 - val_loss: 915723705.0760\n",
      "Epoch 550/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1151031932.6710 - val_loss: 864501229.0682\n",
      "Epoch 551/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1056558361.6229 - val_loss: 869555536.5848\n",
      "Epoch 552/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1126134627.2458 - val_loss: 868611621.7466\n",
      "Epoch 553/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1111380482.9961 - val_loss: 873330900.6199\n",
      "Epoch 554/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1099378452.7230 - val_loss: 867697794.4561\n",
      "Epoch 555/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1086329528.7802 - val_loss: 840141962.4639\n",
      "Epoch 556/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1115391641.3004 - val_loss: 852215120.6862\n",
      "Epoch 557/1000\n",
      "1538/1538 [==============================] - 0s 74us/step - loss: 1205474811.6723 - val_loss: 854898735.4074\n",
      "Epoch 558/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1168725172.7022 - val_loss: 817481806.4094\n",
      "Epoch 559/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1050386149.9090 - val_loss: 831418785.0955\n",
      "Epoch 560/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1113844060.0052 - val_loss: 822657667.7895\n",
      "Epoch 561/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1017635479.3862 - val_loss: 810990357.1462\n",
      "Epoch 562/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 996736610.4863 - val_loss: 890554109.2554\n",
      "Epoch 563/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1153239277.7737 - val_loss: 885661483.6647\n",
      "Epoch 564/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1082101828.3277 - val_loss: 783288163.4620\n",
      "Epoch 565/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1003838975.6255 - val_loss: 796005163.0977\n",
      "Epoch 566/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1081927419.4642 - val_loss: 905356762.9396\n",
      "Epoch 567/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1191614272.7490 - val_loss: 850723977.9571\n",
      "Epoch 568/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1150459741.5449 - val_loss: 863737404.0078\n",
      "Epoch 569/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1427314637.4824 - val_loss: 2457658055.6101\n",
      "Epoch 570/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 2845225060.0364 - val_loss: 2430135952.7173\n",
      "Epoch 571/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 2364226996.4317 - val_loss: 1306141378.6199\n",
      "Epoch 572/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1366126372.2861 - val_loss: 1037336565.2710\n",
      "Epoch 573/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1450815663.7711 - val_loss: 1206354006.3314\n",
      "Epoch 574/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1456064265.3027 - val_loss: 1056590641.9025\n",
      "Epoch 575/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1360245435.4226 - val_loss: 834743426.6199\n",
      "Epoch 576/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1129630119.7607 - val_loss: 886250856.0468\n",
      "Epoch 577/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1223821224.2809 - val_loss: 920251911.7349\n",
      "Epoch 578/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1193630694.9493 - val_loss: 812460123.5712\n",
      "Epoch 579/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1125291359.5423 - val_loss: 835284170.5107\n",
      "Epoch 580/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1120884678.7828 - val_loss: 849291433.1618\n",
      "Epoch 581/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1110408026.2159 - val_loss: 808621961.7466\n",
      "Epoch 582/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1119833354.7776 - val_loss: 792568278.7368\n",
      "Epoch 583/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 965983685.6593 - val_loss: 785210083.3996\n",
      "Epoch 584/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1014696087.6359 - val_loss: 779869508.5536\n",
      "Epoch 585/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1075233128.6970 - val_loss: 794896452.6160\n",
      "Epoch 586/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 993931901.7529 - val_loss: 790485150.8148\n",
      "Epoch 587/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 990368269.3254 - val_loss: 803349444.9903\n",
      "Epoch 588/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1063227508.7646 - val_loss: 787504134.3626\n",
      "Epoch 589/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1062940489.7373 - val_loss: 783991225.6374\n",
      "Epoch 590/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1104673308.1717 - val_loss: 791432044.1637\n",
      "Epoch 591/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1075909155.9532 - val_loss: 785937476.4912\n",
      "Epoch 592/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1065611541.9818 - val_loss: 791394388.3977\n",
      "Epoch 593/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1046524335.6463 - val_loss: 788615140.9279\n",
      "Epoch 594/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1123401130.7776 - val_loss: 778693658.4483\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 70us/step - loss: 1085730419.0585 - val_loss: 816358448.7797\n",
      "Epoch 596/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1065408432.3953 - val_loss: 809283311.6569\n",
      "Epoch 597/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1048357398.9701 - val_loss: 762067876.8655\n",
      "Epoch 598/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1013083681.3316 - val_loss: 736302877.5673\n",
      "Epoch 599/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1035424198.9701 - val_loss: 745832623.5945\n",
      "Epoch 600/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1093977819.2562 - val_loss: 719027775.2515\n",
      "Epoch 601/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1047227109.2016 - val_loss: 699025019.4464\n",
      "Epoch 602/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 983896776.4473 - val_loss: 683735922.4951\n",
      "Epoch 603/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1055617740.0676 - val_loss: 672289438.2534\n",
      "Epoch 604/1000\n",
      "1538/1538 [==============================] - 0s 79us/step - loss: 1002355515.4226 - val_loss: 676078484.6472\n",
      "Epoch 605/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1058781272.0520 - val_loss: 686382141.8168\n",
      "Epoch 606/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 885868112.1456 - val_loss: 702889859.1813\n",
      "Epoch 607/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 992036986.1743 - val_loss: 702631314.7758\n",
      "Epoch 608/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1126246013.8362 - val_loss: 801411192.6082\n",
      "Epoch 609/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1098622577.1339 - val_loss: 741403036.7563\n",
      "Epoch 610/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1083418739.7607 - val_loss: 731247235.8674\n",
      "Epoch 611/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1041493406.8765 - val_loss: 767048857.9493\n",
      "Epoch 612/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1089074246.4915 - val_loss: 758109285.4269\n",
      "Epoch 613/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1016808925.1079 - val_loss: 735393132.2885\n",
      "Epoch 614/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 995688452.0780 - val_loss: 729936839.2359\n",
      "Epoch 615/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1024232190.0026 - val_loss: 729334474.0429\n",
      "Epoch 616/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1067957859.5371 - val_loss: 768591815.8596\n",
      "Epoch 617/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1176740344.6762 - val_loss: 854482742.8538\n",
      "Epoch 618/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1107164667.8388 - val_loss: 745273527.8285\n",
      "Epoch 619/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1014491418.7152 - val_loss: 766690437.4893\n",
      "Epoch 620/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1005609113.8414 - val_loss: 771455508.9591\n",
      "Epoch 621/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1082805016.1769 - val_loss: 756883062.4250\n",
      "Epoch 622/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1051276913.0195 - val_loss: 777559834.4483\n",
      "Epoch 623/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1120661097.1964 - val_loss: 763064798.0039\n",
      "Epoch 624/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1175084889.1339 - val_loss: 1199774245.9259\n",
      "Epoch 625/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1354586660.2237 - val_loss: 1158405317.1150\n",
      "Epoch 626/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1409890120.4369 - val_loss: 864913126.3626\n",
      "Epoch 627/1000\n",
      "1538/1538 [==============================] - 0s 56us/step - loss: 1114139751.1990 - val_loss: 815673727.2076\n",
      "Epoch 628/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1185831040.6242 - val_loss: 1054660571.3840\n",
      "Epoch 629/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1389434761.2380 - val_loss: 861046414.1209\n",
      "Epoch 630/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1086041515.0273 - val_loss: 823520386.8070\n",
      "Epoch 631/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1128647167.0013 - val_loss: 1138461551.4074\n",
      "Epoch 632/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1409552706.4343 - val_loss: 1008420036.4912\n",
      "Epoch 633/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1165110274.6008 - val_loss: 800457425.2164\n",
      "Epoch 634/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1156579084.9831 - val_loss: 776918152.8070\n",
      "Epoch 635/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1009800884.9311 - val_loss: 784004436.6745\n",
      "Epoch 636/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 992083733.0247 - val_loss: 806857893.8012\n",
      "Epoch 637/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1073198602.0624 - val_loss: 837965815.1423\n",
      "Epoch 638/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1112383102.7516 - val_loss: 787839898.1988\n",
      "Epoch 639/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1034705800.8218 - val_loss: 810784785.4659\n",
      "Epoch 640/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1183000203.6931 - val_loss: 1139119981.0370\n",
      "Epoch 641/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1329497152.4577 - val_loss: 1055252810.3548\n",
      "Epoch 642/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 1171506653.8778 - val_loss: 830028160.8733\n",
      "Epoch 643/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1085784597.2536 - val_loss: 879634313.7183\n",
      "Epoch 644/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1240759103.8335 - val_loss: 911633837.5680\n",
      "Epoch 645/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1267968844.4005 - val_loss: 815215263.1384\n",
      "Epoch 646/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1069719766.2211 - val_loss: 864170096.7173\n",
      "Epoch 647/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1170929652.1508 - val_loss: 872082364.6940\n",
      "Epoch 648/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1052454658.3303 - val_loss: 845078100.7096\n",
      "Epoch 649/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1077422733.6073 - val_loss: 796597734.2144\n",
      "Epoch 650/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1070499962.1743 - val_loss: 758273114.6355\n",
      "Epoch 651/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1254235684.6190 - val_loss: 973596446.9396\n",
      "Epoch 652/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1392028822.8869 - val_loss: 997865286.7368\n",
      "Epoch 653/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1266025689.3836 - val_loss: 779102018.3704\n",
      "Epoch 654/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1009613377.6541 - val_loss: 735291334.8811\n",
      "Epoch 655/1000\n",
      "1538/1538 [==============================] - 0s 78us/step - loss: 1079683250.2471 - val_loss: 794562543.5439\n",
      "Epoch 656/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1105866714.0494 - val_loss: 765691344.6004\n",
      "Epoch 657/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1076401494.2211 - val_loss: 739054219.8363\n",
      "Epoch 658/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1206294623.9584 - val_loss: 967196548.5536\n",
      "Epoch 659/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1290621068.8166 - val_loss: 737761106.8304\n",
      "Epoch 660/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1030539122.2679 - val_loss: 901533524.8655\n",
      "Epoch 661/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 70us/step - loss: 1196993840.3953 - val_loss: 1003850303.1891\n",
      "Epoch 662/1000\n",
      "1538/1538 [==============================] - 0s 79us/step - loss: 1246332119.7191 - val_loss: 821275419.6335\n",
      "Epoch 663/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1056460520.3641 - val_loss: 870066339.6491\n",
      "Epoch 664/1000\n",
      "1538/1538 [==============================] - 0s 78us/step - loss: 1629064655.3966 - val_loss: 3308458698.6043\n",
      "Epoch 665/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 4811444094.3355 - val_loss: 3020244420.1170\n",
      "Epoch 666/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 2813617794.2887 - val_loss: 986698916.5536\n",
      "Epoch 667/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1367606581.5969 - val_loss: 1545991913.5439\n",
      "Epoch 668/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1957847978.1118 - val_loss: 1560558887.5478\n",
      "Epoch 669/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1579137416.6554 - val_loss: 995056710.2066\n",
      "Epoch 670/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1439749290.2783 - val_loss: 929863329.0682\n",
      "Epoch 671/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1545628331.2276 - val_loss: 1049455246.0351\n",
      "Epoch 672/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1333591255.3030 - val_loss: 869483782.1988\n",
      "Epoch 673/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1219043846.4083 - val_loss: 749776800.2963\n",
      "Epoch 674/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1092863600.1873 - val_loss: 771788941.0214\n",
      "Epoch 675/1000\n",
      "1538/1538 [==============================] - 0s 57us/step - loss: 1101730510.8140 - val_loss: 738380832.8421\n",
      "Epoch 676/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1073337163.2770 - val_loss: 742842895.3704\n",
      "Epoch 677/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1061736030.3771 - val_loss: 734417779.1092\n",
      "Epoch 678/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1086372814.7308 - val_loss: 799899620.9279\n",
      "Epoch 679/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1135869348.2445 - val_loss: 828504832.5302\n",
      "Epoch 680/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1064703418.5904 - val_loss: 783435206.6433\n",
      "Epoch 681/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1058519124.6398 - val_loss: 719145471.2769\n",
      "Epoch 682/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1108317091.2250 - val_loss: 739705944.2573\n",
      "Epoch 683/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1045920969.9038 - val_loss: 710764896.8246\n",
      "Epoch 684/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1111460328.7802 - val_loss: 763654890.5341\n",
      "Epoch 685/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1058997264.9779 - val_loss: 839810245.5828\n",
      "Epoch 686/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1168953925.2432 - val_loss: 752593517.3021\n",
      "Epoch 687/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 981939429.8674 - val_loss: 767342086.5380\n",
      "Epoch 688/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1198408756.7646 - val_loss: 878394062.2534\n",
      "Epoch 689/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1078849477.3056 - val_loss: 793404202.0370\n",
      "Epoch 690/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1098055421.6905 - val_loss: 750134676.1326\n",
      "Epoch 691/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1048527157.8466 - val_loss: 766453625.8558\n",
      "Epoch 692/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1065096670.2107 - val_loss: 765984604.5380\n",
      "Epoch 693/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1019806659.2146 - val_loss: 904432831.3236\n",
      "Epoch 694/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1233402399.7087 - val_loss: 850932344.2036\n",
      "Epoch 695/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1174881259.5683 - val_loss: 767836793.2632\n",
      "Epoch 696/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 981228889.8830 - val_loss: 840442375.7349\n",
      "Epoch 697/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1078865630.4291 - val_loss: 816433724.1326\n",
      "Epoch 698/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1021564214.2211 - val_loss: 776365819.7583\n",
      "Epoch 699/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1094935149.1912 - val_loss: 780815075.9922\n",
      "Epoch 700/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1057560831.1157 - val_loss: 876916066.3080\n",
      "Epoch 701/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1141562026.1118 - val_loss: 888796298.2300\n",
      "Epoch 702/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1038909244.7958 - val_loss: 883453131.8519\n",
      "Epoch 703/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1031648104.2809 - val_loss: 891757897.8558\n",
      "Epoch 704/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1134675012.3277 - val_loss: 873707729.0916\n",
      "Epoch 705/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1041107960.6762 - val_loss: 879420210.6511\n",
      "Epoch 706/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1267353963.8179 - val_loss: 1412839209.7310\n",
      "Epoch 707/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1781432034.9623 - val_loss: 1048624808.6082\n",
      "Epoch 708/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1340773110.1795 - val_loss: 853858199.9532\n",
      "Epoch 709/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1134466812.5046 - val_loss: 1055362024.0468\n",
      "Epoch 710/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1178081739.2770 - val_loss: 883876330.2924\n",
      "Epoch 711/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1127696396.0676 - val_loss: 849114465.1852\n",
      "Epoch 712/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1109013636.1196 - val_loss: 873238523.3840\n",
      "Epoch 713/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1049769800.4057 - val_loss: 817904661.0838\n",
      "Epoch 714/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1134696317.6957 - val_loss: 823282831.4698\n",
      "Epoch 715/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1004294677.7113 - val_loss: 846238510.1598\n",
      "Epoch 716/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1024189938.8140 - val_loss: 787125418.9162\n",
      "Epoch 717/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1026507489.7113 - val_loss: 758615676.0078\n",
      "Epoch 718/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1051396212.1821 - val_loss: 753240386.6199\n",
      "Epoch 719/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1089826101.2016 - val_loss: 953749128.9825\n",
      "Epoch 720/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1249094470.9077 - val_loss: 879279568.8421\n",
      "Epoch 721/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1112560607.3758 - val_loss: 752787349.8947\n",
      "Epoch 722/1000\n",
      "1538/1538 [==============================] - 0s 93us/step - loss: 985036294.3459 - val_loss: 797002399.1267\n",
      "Epoch 723/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1043181926.2003 - val_loss: 750013494.2690\n",
      "Epoch 724/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1162722002.6424 - val_loss: 916598609.3411\n",
      "Epoch 725/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1161788453.3680 - val_loss: 920122694.8616\n",
      "Epoch 726/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1175876945.8101 - val_loss: 773324249.5750\n",
      "Epoch 727/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 70us/step - loss: 1087586956.3173 - val_loss: 713083195.1969\n",
      "Epoch 728/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1079893898.4863 - val_loss: 705967658.6043\n",
      "Epoch 729/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1021289796.5774 - val_loss: 697270084.1170\n",
      "Epoch 730/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1028042980.5358 - val_loss: 697422455.7661\n",
      "Epoch 731/1000\n",
      "1538/1538 [==============================] - 0s 80us/step - loss: 1007237771.1521 - val_loss: 690513409.6842\n",
      "Epoch 732/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 992219344.5852 - val_loss: 692127473.4659\n",
      "Epoch 733/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1043848124.6710 - val_loss: 698042967.7505\n",
      "Epoch 734/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 993440618.2367 - val_loss: 696784042.8616\n",
      "Epoch 735/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 957363089.8518 - val_loss: 693681217.0682\n",
      "Epoch 736/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 992227785.9870 - val_loss: 693215827.1754\n",
      "Epoch 737/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1003081817.4668 - val_loss: 710539777.2074\n",
      "Epoch 738/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 989361912.4551 - val_loss: 703187633.9864\n",
      "Epoch 739/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1052873144.0936 - val_loss: 709598299.2593\n",
      "Epoch 740/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 981808378.2575 - val_loss: 734660401.5283\n",
      "Epoch 741/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 997476051.2250 - val_loss: 711048682.1676\n",
      "Epoch 742/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 976579520.9987 - val_loss: 716244989.2242\n",
      "Epoch 743/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1102450470.3251 - val_loss: 734378092.5380\n",
      "Epoch 744/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1058961403.3186 - val_loss: 739763183.6101\n",
      "Epoch 745/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1071663465.3628 - val_loss: 743106050.9318\n",
      "Epoch 746/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1014948573.5449 - val_loss: 762134073.8558\n",
      "Epoch 747/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1172670668.4421 - val_loss: 768969215.9688\n",
      "Epoch 748/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1089639205.1183 - val_loss: 725830186.7446\n",
      "Epoch 749/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 899090156.2783 - val_loss: 706157112.8187\n",
      "Epoch 750/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 997285536.9077 - val_loss: 720290497.0039\n",
      "Epoch 751/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1060164256.9155 - val_loss: 709297300.5380\n",
      "Epoch 752/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 928932733.9610 - val_loss: 732189938.5575\n",
      "Epoch 753/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 985229020.2965 - val_loss: 762948410.2612\n",
      "Epoch 754/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1044989741.3576 - val_loss: 758608985.2632\n",
      "Epoch 755/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 961761486.1170 - val_loss: 740155292.6940\n",
      "Epoch 756/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1030383667.0169 - val_loss: 728356441.9805\n",
      "Epoch 757/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1038179941.8674 - val_loss: 884463278.0351\n",
      "Epoch 758/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1393769452.8583 - val_loss: 1947773604.6784\n",
      "Epoch 759/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 2059779666.5592 - val_loss: 1720688631.0175\n",
      "Epoch 760/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1661576985.6333 - val_loss: 830175959.0799\n",
      "Epoch 761/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1120180812.1508 - val_loss: 1138822848.4678\n",
      "Epoch 762/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1458078426.7152 - val_loss: 1017401352.2807\n",
      "Epoch 763/1000\n",
      "1538/1538 [==============================] - 0s 87us/step - loss: 1203587584.0000 - val_loss: 966365165.6608\n",
      "Epoch 764/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1701320642.0806 - val_loss: 2969160505.3879\n",
      "Epoch 765/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 3042140325.1183 - val_loss: 2049449400.6394\n",
      "Epoch 766/1000\n",
      "1538/1538 [==============================] - 0s 84us/step - loss: 1709574683.7971 - val_loss: 916045709.5984\n",
      "Epoch 767/1000\n",
      "1538/1538 [==============================] - 0s 86us/step - loss: 1496057819.7971 - val_loss: 1687400756.0234\n",
      "Epoch 768/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 2164222511.4382 - val_loss: 1219401412.0702\n",
      "Epoch 769/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1429001774.6060 - val_loss: 941244607.6881\n",
      "Epoch 770/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1221025597.5865 - val_loss: 1121017426.0897\n",
      "Epoch 771/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1422283618.2367 - val_loss: 982589015.7037\n",
      "Epoch 772/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1073378876.7126 - val_loss: 855456057.6998\n",
      "Epoch 773/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1177210140.5878 - val_loss: 869731146.0409\n",
      "Epoch 774/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1192548519.7815 - val_loss: 825948898.7680\n",
      "Epoch 775/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1054343909.2016 - val_loss: 958377031.8596\n",
      "Epoch 776/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1205237633.6229 - val_loss: 929308001.8090\n",
      "Epoch 777/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1102002234.5904 - val_loss: 816097267.1813\n",
      "Epoch 778/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1131722143.4590 - val_loss: 803852321.7154\n",
      "Epoch 779/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1120609804.0260 - val_loss: 888385442.6823\n",
      "Epoch 780/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1141052417.6645 - val_loss: 834153674.8538\n",
      "Epoch 781/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1103697245.0338 - val_loss: 852291269.4269\n",
      "Epoch 782/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 1192154133.9714 - val_loss: 928271664.3743\n",
      "Epoch 783/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1241552975.7295 - val_loss: 994080633.4503\n",
      "Epoch 784/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1204168308.7750 - val_loss: 763954221.5984\n",
      "Epoch 785/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 926186354.0182 - val_loss: 769707581.5049\n",
      "Epoch 786/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1049559400.6970 - val_loss: 741045407.9376\n",
      "Epoch 787/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 979003787.2354 - val_loss: 739158930.2768\n",
      "Epoch 788/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1096007715.0377 - val_loss: 840220324.9279\n",
      "Epoch 789/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1190370601.5293 - val_loss: 952898717.6920\n",
      "Epoch 790/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1265923686.3875 - val_loss: 819574361.9493\n",
      "Epoch 791/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1026981604.5358 - val_loss: 767061907.5244\n",
      "Epoch 792/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1105150573.0247 - val_loss: 801908706.2612\n",
      "Epoch 793/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 68us/step - loss: 1059885134.7308 - val_loss: 796689069.5205\n",
      "Epoch 794/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1077464787.8492 - val_loss: 767419935.5010\n",
      "Epoch 795/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1019831863.9272 - val_loss: 748189112.2027\n",
      "Epoch 796/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 953707918.4811 - val_loss: 790960264.6323\n",
      "Epoch 797/1000\n",
      "1538/1538 [==============================] - 0s 79us/step - loss: 1045804463.4382 - val_loss: 776448107.6004\n",
      "Epoch 798/1000\n",
      "1538/1538 [==============================] - 0s 97us/step - loss: 1206130819.6619 - val_loss: 740179101.9727\n",
      "Epoch 799/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1004367289.0091 - val_loss: 778455875.3372\n",
      "Epoch 800/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 932677045.1808 - val_loss: 799498329.0448\n",
      "Epoch 801/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 898255059.7243 - val_loss: 736292684.7173\n",
      "Epoch 802/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1093060645.2848 - val_loss: 802931261.5731\n",
      "Epoch 803/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1028917767.3238 - val_loss: 883351137.4269\n",
      "Epoch 804/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1173532086.3667 - val_loss: 879240665.4981\n",
      "Epoch 805/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1076296752.1040 - val_loss: 806667585.4347\n",
      "Epoch 806/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 931859042.1638 - val_loss: 856877431.8285\n",
      "Epoch 807/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1150735962.3823 - val_loss: 870255357.9415\n",
      "Epoch 808/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1143481347.3290 - val_loss: 854015335.6413\n",
      "Epoch 809/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1206842397.6281 - val_loss: 1165868213.5984\n",
      "Epoch 810/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1407538002.1118 - val_loss: 1168397473.6296\n",
      "Epoch 811/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1239279049.7373 - val_loss: 915795690.3236\n",
      "Epoch 812/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 981990293.8466 - val_loss: 929056112.4678\n",
      "Epoch 813/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1046815535.3134 - val_loss: 1020329343.8752\n",
      "Epoch 814/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1085660511.8752 - val_loss: 945759397.8012\n",
      "Epoch 815/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 991514354.8505 - val_loss: 958153405.0370\n",
      "Epoch 816/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1134635436.2757 - val_loss: 1021710180.9864\n",
      "Epoch 817/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1180374935.3446 - val_loss: 876200452.2105\n",
      "Epoch 818/1000\n",
      "1538/1538 [==============================] - 0s 59us/step - loss: 1066132553.1964 - val_loss: 917319295.1267\n",
      "Epoch 819/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1192568213.1391 - val_loss: 882524083.7739\n",
      "Epoch 820/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1016881409.2900 - val_loss: 839293334.8265\n",
      "Epoch 821/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 949507473.8518 - val_loss: 853991876.5351\n",
      "Epoch 822/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1037183684.9623 - val_loss: 843881729.0068\n",
      "Epoch 823/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 996785032.4889 - val_loss: 837148617.1024\n",
      "Epoch 824/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1091033163.9012 - val_loss: 897464348.3197\n",
      "Epoch 825/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1139912687.8544 - val_loss: 824963281.5426\n",
      "Epoch 826/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1100159076.5358 - val_loss: 942094544.2807\n",
      "Epoch 827/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 1266831423.9168 - val_loss: 1615984809.6686\n",
      "Epoch 828/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1833571435.1521 - val_loss: 1146866554.6355\n",
      "Epoch 829/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1240600466.0598 - val_loss: 792425191.4447\n",
      "Epoch 830/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1055265545.6957 - val_loss: 854926784.2495\n",
      "Epoch 831/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1207791348.3485 - val_loss: 847535313.2788\n",
      "Epoch 832/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1118194715.6827 - val_loss: 760017861.8713\n",
      "Epoch 833/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1087510645.8466 - val_loss: 787605710.7290\n",
      "Epoch 834/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1114152124.7542 - val_loss: 722111529.6062\n",
      "Epoch 835/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1141852623.8127 - val_loss: 757374510.5965\n",
      "Epoch 836/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1122158253.9402 - val_loss: 779996168.9201\n",
      "Epoch 837/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 1131084575.1261 - val_loss: 724108872.9825\n",
      "Epoch 838/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1113500256.5410 - val_loss: 714567844.1793\n",
      "Epoch 839/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1196362695.8231 - val_loss: 835336399.5088\n",
      "Epoch 840/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1258875012.4733 - val_loss: 1054804705.7476\n",
      "Epoch 841/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1375542428.4629 - val_loss: 893269614.7476\n",
      "Epoch 842/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1151669017.6333 - val_loss: 678167283.2749\n",
      "Epoch 843/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1181387784.3225 - val_loss: 855558216.8577\n",
      "Epoch 844/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1335032745.4460 - val_loss: 703225900.6004\n",
      "Epoch 845/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1117292096.5826 - val_loss: 722051377.1930\n",
      "Epoch 846/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1051803596.2266 - val_loss: 797297401.3752\n",
      "Epoch 847/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1193117168.3537 - val_loss: 749218205.8594\n",
      "Epoch 848/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1077363910.1482 - val_loss: 681985018.3704\n",
      "Epoch 849/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1039854210.4967 - val_loss: 706511337.3567\n",
      "Epoch 850/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1091902345.4044 - val_loss: 717111803.6335\n",
      "Epoch 851/1000\n",
      "1538/1538 [==============================] - 0s 81us/step - loss: 1085554539.7971 - val_loss: 679235420.1326\n",
      "Epoch 852/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1059556673.5813 - val_loss: 687485416.7829\n",
      "Epoch 853/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1077618376.9883 - val_loss: 673333704.6238\n",
      "Epoch 854/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1072100280.4265 - val_loss: 683333653.0448\n",
      "Epoch 855/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 1040099157.0143 - val_loss: 668022488.4990\n",
      "Epoch 856/1000\n",
      "1538/1538 [==============================] - 0s 58us/step - loss: 971072574.7724 - val_loss: 658988973.9766\n",
      "Epoch 857/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 966423574.4707 - val_loss: 667317825.9961\n",
      "Epoch 858/1000\n",
      "1538/1538 [==============================] - 0s 74us/step - loss: 1049765199.8960 - val_loss: 724407305.3567\n",
      "Epoch 859/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 70us/step - loss: 1118712715.4018 - val_loss: 661102977.9649\n",
      "Epoch 860/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 939624730.7984 - val_loss: 678226128.0000\n",
      "Epoch 861/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1076013217.2900 - val_loss: 719513024.5302\n",
      "Epoch 862/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1058418247.6931 - val_loss: 746520751.5634\n",
      "Epoch 863/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1085466177.2484 - val_loss: 735883690.7212\n",
      "Epoch 864/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1268864963.7451 - val_loss: 1430153711.4074\n",
      "Epoch 865/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1668374285.6489 - val_loss: 1072678119.2982\n",
      "Epoch 866/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1205125542.9493 - val_loss: 782499220.3821\n",
      "Epoch 867/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1200651738.4655 - val_loss: 1292183851.6647\n",
      "Epoch 868/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1542322111.0845 - val_loss: 829544828.8070\n",
      "Epoch 869/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1153158916.8270 - val_loss: 1021246220.8499\n",
      "Epoch 870/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1473612419.2770 - val_loss: 1485636360.7329\n",
      "Epoch 871/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1642950357.5137 - val_loss: 1101591917.7856\n",
      "Epoch 872/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1119610705.8934 - val_loss: 789390070.8655\n",
      "Epoch 873/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1157085656.3485 - val_loss: 1083625602.9942\n",
      "Epoch 874/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1347883706.4031 - val_loss: 953347614.3158\n",
      "Epoch 875/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1089162552.7594 - val_loss: 757318383.0877\n",
      "Epoch 876/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1038060958.2107 - val_loss: 849186263.3918\n",
      "Epoch 877/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1113437331.8075 - val_loss: 878554313.9181\n",
      "Epoch 878/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1067046860.7334 - val_loss: 837121559.8908\n",
      "Epoch 879/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1204814176.5410 - val_loss: 796857228.6004\n",
      "Epoch 880/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1057406948.3693 - val_loss: 744968679.1267\n",
      "Epoch 881/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 982482581.4720 - val_loss: 794058764.9981\n",
      "Epoch 882/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 973137753.7165 - val_loss: 771623182.2846\n",
      "Epoch 883/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1044925827.2666 - val_loss: 857501134.1111\n",
      "Epoch 884/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1180440158.5644 - val_loss: 754209337.5136\n",
      "Epoch 885/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 974684544.7490 - val_loss: 742466202.6043\n",
      "Epoch 886/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 991432114.4343 - val_loss: 919633852.1326\n",
      "Epoch 887/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1235910981.4096 - val_loss: 940618645.4581\n",
      "Epoch 888/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1075038183.6151 - val_loss: 733035555.2749\n",
      "Epoch 889/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1014200911.8127 - val_loss: 747770515.7295\n",
      "Epoch 890/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1093982503.3654 - val_loss: 854776845.1462\n",
      "Epoch 891/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1222270980.4941 - val_loss: 702699423.2234\n",
      "Epoch 892/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 983225919.6177 - val_loss: 896940183.5789\n",
      "Epoch 893/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1251526515.6619 - val_loss: 992159041.3723\n",
      "Epoch 894/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1216483547.0481 - val_loss: 749283557.5205\n",
      "Epoch 895/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1065649664.0000 - val_loss: 718427077.5192\n",
      "Epoch 896/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1055196080.7698 - val_loss: 691960881.4420\n",
      "Epoch 897/1000\n",
      "1538/1538 [==============================] - 0s 109us/step - loss: 984755326.2939 - val_loss: 692296641.5397\n",
      "Epoch 898/1000\n",
      "1538/1538 [==============================] - 0s 144us/step - loss: 988615893.1697 - val_loss: 688954216.7559\n",
      "Epoch 899/1000\n",
      "1538/1538 [==============================] - 0s 99us/step - loss: 1079856574.8349 - val_loss: 674033899.1540\n",
      "Epoch 900/1000\n",
      "1538/1538 [==============================] - 0s 98us/step - loss: 1021186539.3602 - val_loss: 675465072.9045\n",
      "Epoch 901/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 993525983.0429 - val_loss: 670812627.2144\n",
      "Epoch 902/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 952519205.4096 - val_loss: 674242021.4859\n",
      "Epoch 903/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 973325030.0338 - val_loss: 667872740.3938\n",
      "Epoch 904/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 981456552.5514 - val_loss: 741184787.2437\n",
      "Epoch 905/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1147269341.3784 - val_loss: 723406434.4016\n",
      "Epoch 906/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 970743685.8257 - val_loss: 686298753.2039\n",
      "Epoch 907/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1002534619.2978 - val_loss: 812974621.6920\n",
      "Epoch 908/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1166358697.9454 - val_loss: 666370740.3821\n",
      "Epoch 909/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1087742183.6983 - val_loss: 912312742.1754\n",
      "Epoch 910/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1169462241.7061 - val_loss: 809317149.6920\n",
      "Epoch 911/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1029478539.1990 - val_loss: 712363476.0936\n",
      "Epoch 912/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 955451200.9155 - val_loss: 809791137.5634\n",
      "Epoch 913/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1088296299.9012 - val_loss: 740473128.6559\n",
      "Epoch 914/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 996830441.6957 - val_loss: 716949384.3431\n",
      "Epoch 915/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 894257465.9246 - val_loss: 743967507.0877\n",
      "Epoch 916/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1061381456.8947 - val_loss: 745358448.5926\n",
      "Epoch 917/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1039192018.0078 - val_loss: 897803751.1189\n",
      "Epoch 918/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1239683601.6021 - val_loss: 784880842.8519\n",
      "Epoch 919/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1136308704.2081 - val_loss: 715240215.2359\n",
      "Epoch 920/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1031205800.1144 - val_loss: 762110369.1540\n",
      "Epoch 921/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1095324991.4174 - val_loss: 750787996.3197\n",
      "Epoch 922/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1034102828.2185 - val_loss: 662586243.4776\n",
      "Epoch 923/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1086490045.7529 - val_loss: 670711515.0785\n",
      "Epoch 924/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1028799586.6736 - val_loss: 649301189.3587\n",
      "Epoch 925/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 62us/step - loss: 908416542.6268 - val_loss: 668799996.9669\n",
      "Epoch 926/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 969758709.5969 - val_loss: 673863326.5809\n",
      "Epoch 927/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1006984442.3407 - val_loss: 690074470.8828\n",
      "Epoch 928/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1032714444.4005 - val_loss: 701265146.1901\n",
      "Epoch 929/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1111995440.0624 - val_loss: 728145003.9142\n",
      "Epoch 930/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1016327060.3069 - val_loss: 705455910.6589\n",
      "Epoch 931/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 942890174.2315 - val_loss: 701292821.7442\n",
      "Epoch 932/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1034321339.9220 - val_loss: 685366688.4964\n",
      "Epoch 933/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1040217811.4330 - val_loss: 718596601.0604\n",
      "Epoch 934/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1100275935.5943 - val_loss: 771928182.5185\n",
      "Epoch 935/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 995961317.4096 - val_loss: 740608091.7271\n",
      "Epoch 936/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 954030924.6086 - val_loss: 688926295.7115\n",
      "Epoch 937/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 933630252.8375 - val_loss: 659282212.2256\n",
      "Epoch 938/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1018568717.6489 - val_loss: 655271109.3885\n",
      "Epoch 939/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 981165054.0026 - val_loss: 663668774.9454\n",
      "Epoch 940/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 922216527.8544 - val_loss: 668546884.3080\n",
      "Epoch 941/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1018130966.9701 - val_loss: 720134736.9045\n",
      "Epoch 942/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1110814366.2523 - val_loss: 1068203792.9669\n",
      "Epoch 943/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1326918463.3342 - val_loss: 901965026.3080\n",
      "Epoch 944/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 1102172912.8322 - val_loss: 688349376.2417\n",
      "Epoch 945/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1020651928.6554 - val_loss: 715858268.4347\n",
      "Epoch 946/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1159935759.6047 - val_loss: 727864894.5478\n",
      "Epoch 947/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 1083856520.9051 - val_loss: 672805164.0156\n",
      "Epoch 948/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 916838969.0299 - val_loss: 886535568.0312\n",
      "Epoch 949/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1175583799.5943 - val_loss: 983952336.2183\n",
      "Epoch 950/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1207061978.0494 - val_loss: 719875514.6043\n",
      "Epoch 951/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1034354147.7763 - val_loss: 841191383.9532\n",
      "Epoch 952/1000\n",
      "1538/1538 [==============================] - 0s 63us/step - loss: 1351877602.7048 - val_loss: 761266863.6569\n",
      "Epoch 953/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 911545228.1508 - val_loss: 713220163.1501\n",
      "Epoch 954/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1037236172.5670 - val_loss: 718382302.0039\n",
      "Epoch 955/1000\n",
      "1538/1538 [==============================] - 0s 76us/step - loss: 1043891180.1925 - val_loss: 761283772.5536\n",
      "Epoch 956/1000\n",
      "1538/1538 [==============================] - 0s 71us/step - loss: 995813126.3667 - val_loss: 809223685.7544\n",
      "Epoch 957/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1095748256.9935 - val_loss: 805016455.7973\n",
      "Epoch 958/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1044895677.9610 - val_loss: 831346983.7973\n",
      "Epoch 959/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 1033706465.3732 - val_loss: 853541478.7368\n",
      "Epoch 960/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1079226864.3537 - val_loss: 909664717.3489\n",
      "Epoch 961/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 1104641213.4200 - val_loss: 833735201.9961\n",
      "Epoch 962/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1059539954.6840 - val_loss: 843340123.2904\n",
      "Epoch 963/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1131692346.2991 - val_loss: 925265491.0760\n",
      "Epoch 964/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1186083581.0663 - val_loss: 850138332.9435\n",
      "Epoch 965/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1084199051.8596 - val_loss: 836462440.4834\n",
      "Epoch 966/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1075971624.0312 - val_loss: 937587342.8460\n",
      "Epoch 967/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 1127913179.7139 - val_loss: 858038517.2710\n",
      "Epoch 968/1000\n",
      "1538/1538 [==============================] - 0s 62us/step - loss: 1024534432.2419 - val_loss: 766620119.8743\n",
      "Epoch 969/1000\n",
      "1538/1538 [==============================] - 0s 65us/step - loss: 1040531609.5501 - val_loss: 865023456.0780\n",
      "Epoch 970/1000\n",
      "1538/1538 [==============================] - 0s 60us/step - loss: 1078337450.4863 - val_loss: 797666829.0858\n",
      "Epoch 971/1000\n",
      "1538/1538 [==============================] - 0s 61us/step - loss: 980957705.3212 - val_loss: 728275576.3821\n",
      "Epoch 972/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 834793224.0104 - val_loss: 718699773.1696\n",
      "Epoch 973/1000\n",
      "1538/1538 [==============================] - 0s 67us/step - loss: 889058430.3355 - val_loss: 716907734.9240\n",
      "Epoch 974/1000\n",
      "1538/1538 [==============================] - 0s 66us/step - loss: 952742572.6450 - val_loss: 712690558.4717\n",
      "Epoch 975/1000\n",
      "1538/1538 [==============================] - 0s 68us/step - loss: 919703120.6450 - val_loss: 704813893.6608\n",
      "Epoch 976/1000\n",
      "1538/1538 [==============================] - 0s 74us/step - loss: 1032643416.5514 - val_loss: 702306895.2827\n",
      "Epoch 977/1000\n",
      "1538/1538 [==============================] - 0s 78us/step - loss: 940715426.4551 - val_loss: 721325548.4756\n",
      "Epoch 978/1000\n",
      "1538/1538 [==============================] - ETA: 0s - loss: 949633002.666 - 0s 111us/step - loss: 949399766.5540 - val_loss: 702209037.8168\n",
      "Epoch 979/1000\n",
      "1538/1538 [==============================] - 0s 80us/step - loss: 1018420406.4291 - val_loss: 906292853.6452\n",
      "Epoch 980/1000\n",
      "1538/1538 [==============================] - 0s 297us/step - loss: 1583737350.4083 - val_loss: 2761740860.3821\n",
      "Epoch 981/1000\n",
      "1538/1538 [==============================] - 0s 278us/step - loss: 2886670058.5280 - val_loss: 2182633882.1988\n",
      "Epoch 982/1000\n",
      "1538/1538 [==============================] - 0s 69us/step - loss: 2180114142.8765 - val_loss: 824907801.9493\n",
      "Epoch 983/1000\n",
      "1538/1538 [==============================] - 0s 73us/step - loss: 1213695947.4018 - val_loss: 870990144.8421\n",
      "Epoch 984/1000\n",
      "1538/1538 [==============================] - 0s 77us/step - loss: 1397760448.7490 - val_loss: 866774491.2593\n",
      "Epoch 985/1000\n",
      "1538/1538 [==============================] - 0s 88us/step - loss: 1442987839.1678 - val_loss: 842485335.5789\n",
      "Epoch 986/1000\n",
      "1538/1538 [==============================] - 0s 75us/step - loss: 1211095223.4278 - val_loss: 727866106.4537\n",
      "Epoch 987/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1061167838.8765 - val_loss: 832454239.6725\n",
      "Epoch 988/1000\n",
      "1538/1538 [==============================] - 0s 70us/step - loss: 1303148554.6112 - val_loss: 887769129.0760\n",
      "Epoch 989/1000\n",
      "1538/1538 [==============================] - 0s 88us/step - loss: 1244505145.9246 - val_loss: 782214320.4678\n",
      "Epoch 990/1000\n",
      "1538/1538 [==============================] - 0s 84us/step - loss: 1188728443.3394 - val_loss: 721676566.3236\n",
      "Epoch 991/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538/1538 [==============================] - 0s 133us/step - loss: 1117526437.1183 - val_loss: 698185731.2788\n",
      "Epoch 992/1000\n",
      "1538/1538 [==============================] - 0s 64us/step - loss: 1071327387.7971 - val_loss: 699599255.1140\n",
      "Epoch 993/1000\n",
      "1538/1538 [==============================] - 0s 72us/step - loss: 1183813634.3303 - val_loss: 843221346.5887\n",
      "Epoch 994/1000\n",
      "1538/1538 [==============================] - 0s 104us/step - loss: 1346844030.6684 - val_loss: 753022421.8324\n",
      "Epoch 995/1000\n",
      "1538/1538 [==============================] - 0s 138us/step - loss: 1055123419.0065 - val_loss: 810965510.7992\n",
      "Epoch 996/1000\n",
      "1538/1538 [==============================] - 0s 200us/step - loss: 1149640118.5956 - val_loss: 890870969.7622\n",
      "Epoch 997/1000\n",
      "1538/1538 [==============================] - 0s 123us/step - loss: 1184818285.8570 - val_loss: 736535477.2710\n",
      "Epoch 998/1000\n",
      "1538/1538 [==============================] - 0s 129us/step - loss: 1110384197.5761 - val_loss: 754214652.0543\n",
      "Epoch 999/1000\n",
      "1538/1538 [==============================] - 0s 161us/step - loss: 1187110993.6437 - val_loss: 699307969.6764\n",
      "Epoch 1000/1000\n",
      "1538/1538 [==============================] - 0s 90us/step - loss: 1025899716.9103 - val_loss: 733437212.8499\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_ss, y_train, epochs = 1000, batch_size = 512, validation_data = (X_test_ss, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'loss'])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f62aac0b88>]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (20,12))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9414077841111688"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, model.predict(X_train_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8805481882510211"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19250.133883133785"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_train, model.predict(X_train_ss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27082.04346461276"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, model.predict(X_test_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not quite as good as Random Forests, but still not bad at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
